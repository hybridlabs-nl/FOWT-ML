{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#fowt-ml-floating-offshore-wind-turbine-machine-learning-kit","title":"FOWT-ML: Floating Offshore Wind Turbine Machine Learning Kit","text":"<p>FOWT-ML is a generic machine learning toolkit developed for Hyrbid testing of Floating Offshore Wind Turbines (FOWTs). It provides a set of tools and algorithms to facilitate reaserch on machine learning techniques (specifically multi-output regressions) for real-time hybrid testing setups in wind tunnels or wave basins.</p> <p>The package is designed to be flexible and extensible, allowing users to customize and adapt it to their specific needs and requirements. It includes various machine learning algorithms from linear regression to neural networks, as well as tools for data preprocessing, model evaluation and comparison, and model publication.</p>"},{"location":"#multi-output-regression-in-hyrbid-testing","title":"Multi-output regression in hyrbid testing","text":"<p>In real-time hybrid testing of FOWTs, it is often necessary to use numerical models to simulate missing components or dynamics that cannot be physically measured. For example, in wind tunnels, the hydrodynamic forces acting on the floating platform may not be directly measurable, and thus a numerical model is used to estimate these forces based on the measured wind loads and platform motions. On the other hand, in wave basins, the aerodynamic forces on the wind turbine may not be directly measurable, and a numerical model is used to estimate these forces based on the measured wave loads and turbine motions.</p> <p>Machine learning techniques can be employed to predict the missing forces in one lab based on the available measurements from the other lab. This is where multi-output regression comes into play, as it allows for the simultaneous prediction of multiple outputs i.e., the missing forces in 6 degrees of freedom (DOF) of the floating platform.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":""},{"location":"CHANGELOG/#001-1900-12-31","title":"0.0.1 - 1900-12-31","text":""},{"location":"CHANGELOG/#added","title":"Added","text":""},{"location":"CHANGELOG/#removed","title":"Removed","text":""},{"location":"CHANGELOG/#changed","title":"Changed","text":""},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at Hybridlabs. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"CONTRIBUTING/","title":"How to contribute","text":"<p>We welcome any kind of contribution to our software, from simple comment or question to a full fledged pull request. Please read and follow our Code of Conduct.</p> <p>A contribution can be one of the following cases:</p> <ol> <li>you have a question;</li> <li>you think you may have found a bug (including unexpected behavior);</li> <li>you want to make some kind of change to the code base (e.g. to fix a bug, to add a new feature, to update documentation);</li> <li>you want to make a new release of the code base.</li> </ol> <p>The sections below outline the steps in each case.</p>"},{"location":"CONTRIBUTING/#you-have-a-question","title":"You have a question","text":"<ol> <li>use the search functionality in    issues to see if someone    already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue;</li> <li>apply the \"Question\" label; apply other labels when relevant.</li> </ol>"},{"location":"CONTRIBUTING/#you-think-you-may-have-found-a-bug","title":"You think you may have found a bug","text":"<ol> <li>use the search functionality in    issues to see if someone    already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue, making sure to provide enough information to the rest of the community to understand the cause and context of the problem. Depending on the issue, you may want to include:<ul> <li>the SHA hashcode of the commit that is causing your problem;</li> <li>some identifying information (name and version number) for dependencies you're using;</li> <li>information about the operating system;</li> </ul> </li> <li>apply relevant labels to the newly created issue.</li> </ol>"},{"location":"CONTRIBUTING/#you-want-to-make-some-kind-of-change-to-the-code-base","title":"You want to make some kind of change to the code base","text":"<ol> <li>(important) announce your plan to the rest of the community before you start working. This announcement should be in the form of a (new) issue;</li> <li>(important) wait until some kind of consensus is reached about your idea being a good idea;</li> <li>follow the instruction in developer_guide.md.</li> </ol> <p>In case you feel like you've made a valuable contribution, but you don't know how to write or run tests for it, or how to generate the documentation: don't let this discourage you from making the pull request; we can help you! Just go ahead and submit the pull request, but keep in mind that you might be asked to append additional commits to your pull request.</p>"},{"location":"CONTRIBUTING/#you-want-to-make-a-new-release-of-the-code-base","title":"You want to make a new release of the code base","text":"<p>To create a release you need write permission on the repository.</p> <ol> <li>Check the author list in <code>CITATION.cff</code></li> <li>Bump the version using <code>bump-my-version bump &lt;major|minor|patch&gt;</code>. The    version can be manually changed in <code>pyproject.toml</code> in    the root of the repository. Follow Semantic Versioning    principles. Also, update <code>__version__</code> variable in <code>src/fwot_ml/__init__.py</code>    to the same version.</li> <li>Go to the GitHub release    page. Press draft a new    release button. Fill version, title and description field. Press the Publish    Release button. For this package, if the zenodo integration is enabled, a new    DOI will be created automatically.</li> <li>This software automatically publish to PyPI using a release or publish    workflow. Wait until PyPi publish    workflow has completed and verify    new release is on PyPi.</li> </ol>"},{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/#data-modules","title":"Data modules","text":""},{"location":"api_reference/#fowt_ml.datasets","title":"fowt_ml.datasets","text":"<p>This module contains functions to load and preprocess datasets.</p> <p>Functions:</p> <ul> <li> <code>convert_mat_to_df</code>             \u2013              <p>Reads a matlab file and returns a pandas DataFrame.</p> </li> <li> <code>get_data</code>             \u2013              <p>Returns a dataframe for the given data_id.</p> </li> <li> <code>check_data</code>             \u2013              <p>Checks if the dataframe has the required columns and their are valid.</p> </li> <li> <code>fix_column_names</code>             \u2013              <p>Fixes the column names to remove special characters.</p> </li> </ul>"},{"location":"api_reference/#fowt_ml.datasets.convert_mat_to_df","title":"convert_mat_to_df","text":"<pre><code>convert_mat_to_df(mat_file: str, data_id: str) -&gt; DataFrame\n</code></pre> <p>Reads a matlab file and returns a pandas DataFrame.</p> <p>Parameters:</p> <ul> <li> <code>mat_file</code>               (<code>str</code>)           \u2013            <p>Path to a matlab file.</p> </li> <li> <code>data_id</code>               (<code>str</code>)           \u2013            <p>ID of the data in the matlab file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: DataFrame containing the data.</p> </li> </ul> Source code in <code>src/fowt_ml/datasets.py</code> <pre><code>def convert_mat_to_df(mat_file: str, data_id: str) -&gt; pd.DataFrame:\n    \"\"\"Reads a matlab file and returns a pandas DataFrame.\n\n    Args:\n        mat_file (str): Path to a matlab file.\n        data_id (str): ID of the data in the matlab file.\n\n    Returns:\n        pd.DataFrame: DataFrame containing the data.\n    \"\"\"\n    hdf = h5py.File(mat_file, mode=\"r\")\n\n    # validate the file\n    if data_id not in hdf:\n        raise ValueError(f\"Experiment {data_id} not found in the file.\")\n\n    if \"X\" not in hdf[data_id] or \"Y\" not in hdf[data_id]:\n        raise ValueError(f\"Experiment {data_id} does not have X or Y data.\")\n\n    if \"Data\" not in hdf[data_id][\"X\"]:\n        raise ValueError(f\"Experiment {data_id} does not have X Data.\")\n\n    if \"Name\" not in hdf[data_id][\"Y\"] or \"Data\" not in hdf[data_id][\"Y\"]:\n        raise ValueError(f\"Experiment {data_id} does not have Y Name or Data.\")\n\n    data = {\"time\": np.array(hdf[data_id][\"X\"][\"Data\"][:]).flatten()}\n    name_references = np.array(hdf[data_id][\"Y\"][\"Name\"][:]).flatten()\n    data_references = np.array(hdf[data_id][\"Y\"][\"Data\"][:]).flatten()\n\n    for index, (name_ref, data_ref) in enumerate(\n        zip(name_references, data_references, strict=False)\n    ):\n        name = \"\".join([chr(item[0]) for item in hdf[name_ref]])\n        if name in data:\n            msg = (\n                f\"Duplicate name {name} found in the data.\"\n                f\" Renaming it to {name}_{index}.\"\n            )\n            logger.warning(msg)\n            name = f\"{name}_{index}\"\n        data[name] = np.array(hdf[data_ref]).flatten()\n    return pd.DataFrame(data)\n</code></pre>"},{"location":"api_reference/#fowt_ml.datasets.get_data","title":"get_data","text":"<pre><code>get_data(data_id: str, config: dict) -&gt; DataFrame\n</code></pre> <p>Returns a dataframe for the given data_id.</p> <p>Parameters:</p> <ul> <li> <code>data_id</code>               (<code>str</code>)           \u2013            <p>ID of the data in the configuration file.</p> </li> <li> <code>config</code>               (<code>dict</code>)           \u2013            <p>Configuration dictionary. Example: {\"data_id\": {\"path_file\": \"data.mat\"}}.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: DataFrame for the given data_id.</p> </li> </ul> Source code in <code>src/fowt_ml/datasets.py</code> <pre><code>def get_data(data_id: str, config: dict) -&gt; pd.DataFrame:\n    \"\"\"Returns a dataframe for the given data_id.\n\n    Args:\n        data_id (str): ID of the data in the configuration file.\n        config (dict): Configuration dictionary.\n            Example: {\"data_id\": {\"path_file\": \"data.mat\"}}.\n\n    Returns:\n        pd.DataFrame: DataFrame for the given data_id.\n\n    \"\"\"\n    data_info = config[data_id]\n    df = convert_mat_to_df(data_info[\"path_file\"], data_id)\n\n    # check if auxiliary data is present in the config file\n    if \"aux_data\" in data_info:\n        for key, val in data_info[\"aux_data\"].items():\n            if key not in df and val is not None:\n                df[key] = val\n                msg = (\n                    f\"{key} not found in the data file. \"\n                    f\"But found in config file. \"\n                    f\"Setting it to {val}.\"\n                )\n                logger.info(msg)\n    return df\n</code></pre>"},{"location":"api_reference/#fowt_ml.datasets.check_data","title":"check_data","text":"<pre><code>check_data(df: DataFrame, col_names) -&gt; DataFrame\n</code></pre> <p>Checks if the dataframe has the required columns and their are valid.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame to check.</p> </li> <li> <code>col_names</code>               (<code>list</code>)           \u2013            <p>List of required columns.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: DataFrame with valid required columns.</p> </li> </ul> Source code in <code>src/fowt_ml/datasets.py</code> <pre><code>def check_data(df: pd.DataFrame, col_names) -&gt; pd.DataFrame:\n    \"\"\"Checks if the dataframe has the required columns and their are valid.\n\n    Args:\n        df (pd.DataFrame): DataFrame to check.\n        col_names (list): List of required columns.\n\n    Returns:\n        pd.DataFrame: DataFrame with valid required columns.\n\n    \"\"\"\n    missing_columns = [col for col in col_names if col not in df.columns]\n    if missing_columns:\n        raise ValueError(f\"Missing columns: {missing_columns}\")\n\n    # check if the columns have valid values\n    for col in df.columns:\n        if df[col].isnull().any():\n            raise ValueError(f\"Column {col} has NaN values.\")\n        if not np.issubdtype(df[col].dtype, np.number):\n            raise ValueError(f\"Column {col} is not numeric.\")\n\n    return df\n</code></pre>"},{"location":"api_reference/#fowt_ml.datasets.fix_column_names","title":"fix_column_names","text":"<pre><code>fix_column_names(df: DataFrame) -&gt; DataFrame\n</code></pre> <p>Fixes the column names to remove special characters.</p> <p>Parameters:</p> <ul> <li> <code>df</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame to fix.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: DataFrame with fixed column names.</p> </li> </ul> Source code in <code>src/fowt_ml/datasets.py</code> <pre><code>def fix_column_names(df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Fixes the column names to remove special characters.\n\n    Args:\n        df (pd.DataFrame): DataFrame to fix.\n\n    Returns:\n        pd.DataFrame: DataFrame with fixed column names.\n\n    \"\"\"\n    df.rename(\n        columns=lambda col: (\n            col.replace(\"[\", \"_\").replace(\"]\", \"\").replace(\"&lt;\", \"_\").replace(\"&gt;\", \"\")\n        ),\n        inplace=True,\n    )\n    return df\n</code></pre>"},{"location":"api_reference/#config-modules","title":"Config modules","text":""},{"location":"api_reference/#fowt_ml.config","title":"fowt_ml.config","text":"<p>This module contains functions to read configuration files.</p> <p>Classes:</p> <ul> <li> <code>BaseConfig</code>           \u2013            </li> <li> <code>MLConfig</code>           \u2013            </li> <li> <code>Config</code>           \u2013            <p>Base class for configuration files.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>get_allowed_kwargs</code>             \u2013              <p>Return valid keyword args for a function or class constructor.</p> </li> <li> <code>get_config_file</code>             \u2013              <p>Get the config file path.</p> </li> </ul>"},{"location":"api_reference/#fowt_ml.config.BaseConfig","title":"BaseConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Methods:</p> <ul> <li> <code>as_dict</code>             \u2013              <p>Return the config as a (nested) dict.</p> </li> </ul>"},{"location":"api_reference/#fowt_ml.config.BaseConfig.as_dict","title":"as_dict","text":"<pre><code>as_dict(*, by_alias: bool = False) -&gt; dict\n</code></pre> <p>Return the config as a (nested) dict.</p> Source code in <code>src/fowt_ml/config.py</code> <pre><code>def as_dict(self, *, by_alias: bool = False) -&gt; dict:\n    \"\"\"Return the config as a (nested) dict.\"\"\"\n    return self.model_dump(by_alias=by_alias)\n</code></pre>"},{"location":"api_reference/#fowt_ml.config.MLConfig","title":"MLConfig","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Methods:</p> <ul> <li> <code>validate_tts_kwargs</code>             \u2013              <p>Validate train_test_split kwargs.</p> </li> <li> <code>validate_cv_kwargs</code>             \u2013              <p>Validate cross_validate kwargs.</p> </li> <li> <code>validate_models</code>             \u2013              <p>Validate model names and their kwargs.</p> </li> </ul>"},{"location":"api_reference/#fowt_ml.config.MLConfig.validate_tts_kwargs","title":"validate_tts_kwargs  <code>classmethod</code>","text":"<pre><code>validate_tts_kwargs(v: dict[str, Any]) -&gt; dict[str, Any]\n</code></pre> <p>Validate train_test_split kwargs.</p> Source code in <code>src/fowt_ml/config.py</code> <pre><code>@pydantic.field_validator(\"train_test_split_kwargs\")\n@classmethod\ndef validate_tts_kwargs(cls, v: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Validate train_test_split kwargs.\"\"\"\n    allowed_tts_kwargs = get_allowed_kwargs(train_test_split)\n\n    if invalid := set(v.keys()) - allowed_tts_kwargs:\n        raise ValueError(\n            f\"Invalid train_test_split kwargs: {invalid}. \"\n            f\"Allowed: {sorted(allowed_tts_kwargs)}\"\n        )\n    return v\n</code></pre>"},{"location":"api_reference/#fowt_ml.config.MLConfig.validate_cv_kwargs","title":"validate_cv_kwargs  <code>classmethod</code>","text":"<pre><code>validate_cv_kwargs(v: dict[str, Any]) -&gt; dict[str, Any]\n</code></pre> <p>Validate cross_validate kwargs.</p> Source code in <code>src/fowt_ml/config.py</code> <pre><code>@pydantic.field_validator(\"cross_validation_kwargs\")\n@classmethod\ndef validate_cv_kwargs(cls, v: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"Validate cross_validate kwargs.\"\"\"\n    allowed_cv_kwargs = get_allowed_kwargs(cross_validate)\n    if invalid := set(v.keys()) - allowed_cv_kwargs:\n        raise ValueError(\n            f\"Invalid cross_validate kwargs: {invalid}. \"\n            f\"Allowed: {sorted(allowed_cv_kwargs)}\"\n        )\n    return v\n</code></pre>"},{"location":"api_reference/#fowt_ml.config.MLConfig.validate_models","title":"validate_models  <code>classmethod</code>","text":"<pre><code>validate_models(v: dict[str, dict[str, Any]]) -&gt; dict[str, dict[str, Any]]\n</code></pre> <p>Validate model names and their kwargs.</p> Source code in <code>src/fowt_ml/config.py</code> <pre><code>@pydantic.field_validator(\"model_names\")\n@classmethod\ndef validate_models(cls, v: dict[str, dict[str, Any]]) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Validate model names and their kwargs.\"\"\"\n    estimator_map = {\n        name: est_cls\n        for model_class in [\n            LinearModels,\n            EnsembleModel,\n            SparseGaussianModel,\n            NeuralNetwork,\n            XGBoost,\n        ]\n        for name, est_cls in model_class.ESTIMATOR_NAMES.items()\n    }\n\n    for model_name, kwargs in v.items():\n        if model_name not in estimator_map:\n            raise ValueError(\n                f\"Model '{model_name}' not supported. \"\n                f\"Available: {list(estimator_map.keys())}\"\n            )\n\n        # Get the constructor signature for that model class\n        model_class = estimator_map[model_name]\n        allowed_kwargs = get_allowed_kwargs(model_class)\n        if model_name in {\"RNNRegressor\", \"LSTMRegressor\", \"GRURegressor\"}:\n            model_class = create_skorch_regressor\n            allowed_kwargs = get_allowed_kwargs(model_class)\n            model_class = skorch.net.NeuralNet\n            allowed_kwargs = allowed_kwargs | get_allowed_kwargs(model_class)\n\n        if invalid := set(kwargs.keys()) - allowed_kwargs:\n            raise ValueError(\n                f\"Invalid kwargs for model '{model_name}': {invalid}. \"\n                f\"Allowed: {allowed_kwargs}\"\n            )\n    return v\n</code></pre>"},{"location":"api_reference/#fowt_ml.config.Config","title":"Config","text":"<p>               Bases: <code>BaseConfig</code></p> <p>Base class for configuration files.</p> <p>Methods:</p> <ul> <li> <code>from_yaml</code>             \u2013              <p>Read configs from a config.yaml file.</p> </li> <li> <code>to_yaml</code>             \u2013              <p>Write configs to a yaml config_file.</p> </li> </ul>"},{"location":"api_reference/#fowt_ml.config.Config.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(config_file)\n</code></pre> <p>Read configs from a config.yaml file.</p> <p>If key is not found in config.yaml, the default value is used.</p> Source code in <code>src/fowt_ml/config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, config_file):\n    \"\"\"Read configs from a config.yaml file.\n\n    If key is not found in config.yaml, the default value is used.\n    \"\"\"\n    if not Path(config_file).exists():\n        raise FileNotFoundError(f\"Config file {config_file} not found.\")\n\n    with open(config_file) as f:\n        try:\n            cfg = yaml.safe_load(f)\n        except yaml.YAMLError as exc:\n            raise SyntaxError(f\"Error parsing config file {config_file}.\") from exc\n    return cls(**cfg)\n</code></pre>"},{"location":"api_reference/#fowt_ml.config.Config.to_yaml","title":"to_yaml  <code>classmethod</code>","text":"<pre><code>to_yaml(config_file)\n</code></pre> <p>Write configs to a yaml config_file.</p> Source code in <code>src/fowt_ml/config.py</code> <pre><code>@classmethod\ndef to_yaml(cls, config_file):\n    \"\"\"Write configs to a yaml config_file.\"\"\"\n    if Path(config_file).exists():\n        logger.warning(f\"Overwriting config file {config_file}.\")\n\n    cfg = _schema(cls)\n    with open(config_file, \"w\") as f:\n        yaml.dump(cfg, f, sort_keys=False)\n</code></pre>"},{"location":"api_reference/#fowt_ml.config.get_allowed_kwargs","title":"get_allowed_kwargs","text":"<pre><code>get_allowed_kwargs(func_or_class)\n</code></pre> <p>Return valid keyword args for a function or class constructor.</p> Source code in <code>src/fowt_ml/config.py</code> <pre><code>def get_allowed_kwargs(func_or_class):\n    \"\"\"Return valid keyword args for a function or class constructor.\"\"\"\n    if inspect.isclass(func_or_class):\n        # Handle sklearn-style estimators (incl. XGBoost)\n        if hasattr(func_or_class, \"get_params\"):\n            try:\n                return set(func_or_class().get_params().keys())\n            except Exception:\n                pass\n        sig = inspect.signature(func_or_class.__init__)\n    else:\n        sig = inspect.signature(func_or_class)\n    return set(sig.parameters.keys()) - {\"self\", \"kwargs\"}  # drop 'self' and 'kwargs'\n</code></pre>"},{"location":"api_reference/#fowt_ml.config.get_config_file","title":"get_config_file","text":"<pre><code>get_config_file()\n</code></pre> <p>Get the config file path.</p> Source code in <code>src/fowt_ml/config.py</code> <pre><code>def get_config_file():\n    \"\"\"Get the config file path.\"\"\"\n    config_path = Path.home() / \".config\" / \"fowt_ml\"\n    if os.environ.get(\"CONFIG_PATH\"):\n        return os.environ.get(\"CONFIG_PATH\")\n    elif os.path.exists(config_path):\n        yml_files = list(Path.glob(config_path, \"*.yml\"))\n        if len(yml_files) &gt; 1:\n            raise ValueError(\n                f\"Multiple config files found in {config_path}. Please specify one.\"\n            )\n        return config_path / yml_files[0]\n    else:\n        raise FileNotFoundError(\n            f\"Config file not found. Please specify one in {config_path}\"\n            \" or as an environment variable `CONFIG_PATH`.\"\n        )\n</code></pre>"},{"location":"api_reference/#ml-pipelines","title":"ML pipelines","text":""},{"location":"api_reference/#fowt_ml.pipeline","title":"fowt_ml.pipeline","text":"<p>Classes:</p> <ul> <li> <code>Pipeline</code>           \u2013            </li> </ul>"},{"location":"api_reference/#fowt_ml.pipeline.Pipeline","title":"Pipeline","text":"<pre><code>Pipeline(config: str | Config)\n</code></pre> <p>Parameters:</p> <ul> <li> <code>config</code>               (<code>str | Config</code>)           \u2013            <p>Path to the configuration file or a Config object.</p> </li> <li> <code>kwargs</code>           \u2013            <p>Additional keyword arguments to override the configuration file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>           \u2013            <p>None</p> </li> </ul> <p>Methods:</p> <ul> <li> <code>get_data</code>             \u2013              <p>Returns the dataset for the given data_id.</p> </li> <li> <code>train_test_split</code>             \u2013              <p>Splits the data into training and testing sets.</p> </li> <li> <code>get_models</code>             \u2013              <p>Returns the models for the given model names.</p> </li> <li> <code>setup</code>             \u2013              <p>Set up the machine learning experiment.</p> </li> <li> <code>compare_models</code>             \u2013              <p>Compares the models and returns the best model.</p> </li> </ul> Source code in <code>src/fowt_ml/pipeline.py</code> <pre><code>def __init__(self, config: str | Config) -&gt; None:\n    \"\"\"Initializes the machine learning pipeline.\n\n    Args:\n        config (str | Config): Path to the configuration file or a Config object.\n        kwargs: Additional keyword arguments to override the configuration file.\n\n    Returns:\n        None\n    \"\"\"\n    config = config if isinstance(config, Config) else Config.from_yaml(config)\n\n    self.predictors_labels = config[\"ml_setup\"][\"predictors\"]\n    self.target_labels = config[\"ml_setup\"][\"targets\"]\n    self.model_names = config[\"ml_setup\"][\"model_names\"]\n    self.metric_names = config[\"ml_setup\"][\"metric_names\"]\n    self.train_test_split_kwargs = config[\"ml_setup\"][\"train_test_split_kwargs\"]\n    self.cross_validation_kwargs = config[\"ml_setup\"][\"cross_validation_kwargs\"]\n    self.scale_data = config[\"ml_setup\"][\"scale_data\"]\n\n    self.work_dir = Path(config[\"session_setup\"][\"work_dir\"])\n\n    self.data_config = config[\"data\"]\n    self.save_grid_scores = config[\"ml_setup\"][\"save_grid_scores\"]\n    self.save_best_model = config[\"ml_setup\"][\"save_best_model\"]\n\n    self.log_experiment = config[\"ml_setup\"][\"log_experiment\"]\n</code></pre>"},{"location":"api_reference/#fowt_ml.pipeline.Pipeline.get_data","title":"get_data","text":"<pre><code>get_data(data_id: str) -&gt; DataFrame\n</code></pre> <p>Returns the dataset for the given data_id.</p> <p>Parameters:</p> <ul> <li> <code>data_id</code>               (<code>str</code>)           \u2013            <p>ID of the data in the configuration file.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>DataFrame</code>           \u2013            <p>pd.DataFrame: DataFrame for the given data_id, set in the</p> </li> <li> <code>DataFrame</code>           \u2013            <p>configuration file.</p> </li> </ul> Source code in <code>src/fowt_ml/pipeline.py</code> <pre><code>def get_data(self, data_id: str) -&gt; pd.DataFrame:\n    \"\"\"Returns the dataset for the given data_id.\n\n    Args:\n        data_id (str): ID of the data in the configuration file.\n\n    Returns:\n        pd.DataFrame: DataFrame for the given data_id, set in the\n        configuration file.\n    \"\"\"\n    return get_data(data_id, self.data_config)\n</code></pre>"},{"location":"api_reference/#fowt_ml.pipeline.Pipeline.train_test_split","title":"train_test_split","text":"<pre><code>train_test_split(**kwargs)\n</code></pre> <p>Splits the data into training and testing sets.</p> <p>The data should be set in self.data before calling this method. kwargs are passed to sklearn.model_selection.train_test_split.</p> Source code in <code>src/fowt_ml/pipeline.py</code> <pre><code>def train_test_split(self, **kwargs):\n    \"\"\"Splits the data into training and testing sets.\n\n    The data should be set in self.data before calling this method.\n    kwargs are passed to sklearn.model_selection.train_test_split.\n    \"\"\"\n    if not hasattr(self, \"X_data\") or not hasattr(self, \"Y_data\"):\n        raise ValueError(\"Data not found. Run setup before splitting.\")\n\n    return train_test_split(self.X_data, self.Y_data, **kwargs)\n</code></pre>"},{"location":"api_reference/#fowt_ml.pipeline.Pipeline.get_models","title":"get_models","text":"<pre><code>get_models()\n</code></pre> <p>Returns the models for the given model names.</p> <p>Returns:</p> <ul> <li> <code>dict</code>          \u2013            <p>Dictionary of models.</p> </li> </ul> Source code in <code>src/fowt_ml/pipeline.py</code> <pre><code>def get_models(self):\n    \"\"\"Returns the models for the given model names.\n\n    Returns:\n        dict: Dictionary of models.\n    \"\"\"\n    models = {}\n    model_classes = [\n        LinearModels,\n        EnsembleModel,\n        SparseGaussianModel,\n        NeuralNetwork,\n        XGBoost,\n    ]\n    for model_name, kwrags in self.model_names.items():\n        for model_class in model_classes:\n            if model_name in model_class.ESTIMATOR_NAMES:\n                models[model_name] = model_class(model_name, **kwrags)\n                break\n        else:\n            raise ValueError(f\"Model {model_name} not supported.\")\n    return models\n</code></pre>"},{"location":"api_reference/#fowt_ml.pipeline.Pipeline.setup","title":"setup","text":"<pre><code>setup(data: DataFrame | str) -&gt; Any\n</code></pre> <p>Set up the machine learning experiment.</p> <ul> <li>find the data</li> <li>train test split</li> <li>setup the models for comparison</li> </ul> <p>Parameters:</p> <ul> <li> <code>data</code>               (<code>DataFrame</code>)           \u2013            <p>DataFrame containing the data.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>           \u2013            <p>Experiment object or similar.</p> </li> </ul> Source code in <code>src/fowt_ml/pipeline.py</code> <pre><code>def setup(self, data: pd.DataFrame | str) -&gt; Any:\n    \"\"\"Set up the machine learning experiment.\n\n    - find the data\n    - train test split\n    - setup the models for comparison\n\n    Args:\n        data (pd.DataFrame): DataFrame containing the data.\n\n    Returns:\n        Experiment object or similar.\n\n    \"\"\"\n    if isinstance(data, str):\n        data = self.get_data(data)\n\n    # check if the data has the required columns, and valid values\n    data = check_data(data, self.predictors_labels + self.target_labels)\n\n    self.X_data = data.loc[:, self.predictors_labels]\n    self.Y_data = data.loc[:, self.target_labels]\n\n    # convert to numpy arrays for consistency between libraries\n    self.X_data = np.asarray(self.X_data, dtype=np.float32)\n    self.Y_data = np.asarray(self.Y_data, dtype=np.float32)\n\n    self.X_train, self.X_test, self.Y_train, self.Y_test = self.train_test_split(\n        **self.train_test_split_kwargs\n    )\n\n    # get the models\n    self.model_instances = self.get_models()\n\n    # create work directory\n    self.work_dir.mkdir(parents=True, exist_ok=True)\n\n    # setup mlflow if logging is enabled\n    if self.log_experiment:\n        self._setup_mlflow()\n</code></pre>"},{"location":"api_reference/#fowt_ml.pipeline.Pipeline.compare_models","title":"compare_models","text":"<pre><code>compare_models(sort: str = 'r2', cross_validation: bool = False) -&gt; Any\n</code></pre> <p>Compares the models and returns the best model.</p> <p>\"model_fit_time\" is in seconds.</p> <p>Parameters:</p> <ul> <li> <code>sort</code>               (<code>str</code>, default:                   <code>'r2'</code> )           \u2013            <p>Metric to sort the models by. Defaults to \"r2\".</p> </li> <li> <code>cross_validation</code>               (<code>bool</code>, default:                   <code>False</code> )           \u2013            <p>Whether to use cross-validation</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>tuple</code> (              <code>Any</code> )          \u2013            <p>(dict of fitted models, pd.DataFrame of grid scores sorted by <code>sort</code>)</p> </li> </ul> Source code in <code>src/fowt_ml/pipeline.py</code> <pre><code>def compare_models(self, sort: str = \"r2\", cross_validation: bool = False) -&gt; Any:\n    \"\"\"Compares the models and returns the best model.\n\n    \"model_fit_time\" is in seconds.\n\n    Args:\n        sort (str, optional): Metric to sort the models by. Defaults to \"r2\".\n        cross_validation (bool, optional): Whether to use cross-validation\n        for comparison. Defaults to False.\n\n    Returns:\n        tuple: (dict of fitted models, pd.DataFrame of grid scores sorted by `sort`)\n    \"\"\"\n    self.fitted_models = {}\n    self.scores = {}\n    for model_name in self.model_names:\n        fitted_model, scores = self._run_model(model_name, cross_validation)\n        self.fitted_models[model_name] = fitted_model\n        self.scores[model_name] = scores\n\n    grid_scores = pd.DataFrame(self.scores).T\n\n    if sort not in grid_scores.columns:\n        raise ValueError(\n            f\"Sort '{sort}' not in metrics {grid_scores.columns.tolist()}\"\n            \" provided. Choose one of the metrics to sort the models.\"\n        )\n\n    ascending = sort in {\"model_fit_time\", \"model_predict_time\"}\n    self.grid_scores_sorted = grid_scores.sort_values(by=sort, ascending=ascending)\n\n    self._log_model()\n    self._save_grid_scores()\n    self._save_best_model()\n\n    return self.fitted_models, self.grid_scores_sorted\n</code></pre>"},{"location":"api_reference/#model-modules","title":"Model modules","text":""},{"location":"api_reference/#fowt_ml.base","title":"fowt_ml.base","text":"<p>This is the base class for all models in the fowt_ml package.</p> <p>Classes:</p> <ul> <li> <code>BaseModel</code>           \u2013            <p>Base class for all models.</p> </li> </ul>"},{"location":"api_reference/#fowt_ml.base.BaseModel","title":"BaseModel","text":"<pre><code>BaseModel(estimator: str | BaseEstimator, **kwargs: dict[str, Any])\n</code></pre> <p>Base class for all models.</p> <p>Methods:</p> <ul> <li> <code>calculate_score</code>             \u2013              <p>Calculate the score for the model using test data.</p> </li> <li> <code>cross_validate</code>             \u2013              <p>Perform cross-validation on the model.</p> </li> <li> <code>use_scaled_data</code>             \u2013              <p>Wrap the estimator to use scaled data for both X and y.</p> </li> </ul> Source code in <code>src/fowt_ml/base.py</code> <pre><code>def __init__(\n    self, estimator: str | BaseEstimator, **kwargs: dict[str, Any]\n) -&gt; None:\n    \"\"\"Initialize the class with the estimator.\"\"\"\n    if isinstance(estimator, str):\n        if estimator not in self.ESTIMATOR_NAMES:\n            raise ValueError(f\"Available estimators: {self.ESTIMATOR_NAMES.keys()}\")\n        self.estimator = self.ESTIMATOR_NAMES[estimator](**kwargs)\n    else:\n        self.estimator = estimator.set_params(**kwargs)\n</code></pre>"},{"location":"api_reference/#fowt_ml.base.BaseModel.calculate_score","title":"calculate_score","text":"<pre><code>calculate_score(x_train: ArrayLike, x_test: ArrayLike, y_train: ArrayLike, y_test: ArrayLike, scoring: str | Iterable) -&gt; float | dict[str, float]\n</code></pre> <p>Calculate the score for the model using test data.</p> <p>First, the model is fitted to the training data, and the time taken to fit the model is recorded. Then, the model is scored using the provided scoring method(s) on the <code>test</code> data.</p> <p>In multi-output regression, by default, 'uniform_average' is used, which specifies a uniformly weighted mean over outputs. see https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics</p> <p>For scoring paramers overview: https://scikit-learn.org/stable/modules/model_evaluation.html#string-name-scorers</p> <p>Parameters:</p> <ul> <li> <code>x_train</code>               (<code>ArrayLike</code>)           \u2013            <p>training data for features</p> </li> <li> <code>x_test</code>               (<code>ArrayLike</code>)           \u2013            <p>test data for features</p> </li> <li> <code>y_train</code>               (<code>ArrayLike</code>)           \u2013            <p>training data for targets</p> </li> <li> <code>y_test</code>               (<code>ArrayLike</code>)           \u2013            <p>test data for targets</p> </li> <li> <code>scoring</code>               (<code>str | Iterable</code>)           \u2013            <p>scoring method(s) to use.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>float | dict[str, float]</code>           \u2013            <p>float | dict[str, float]: the calculated score(s)</p> </li> </ul> Source code in <code>src/fowt_ml/base.py</code> <pre><code>def calculate_score(\n    self,\n    x_train: ArrayLike,\n    x_test: ArrayLike,\n    y_train: ArrayLike,\n    y_test: ArrayLike,\n    scoring: str | Iterable,\n) -&gt; float | dict[str, float]:\n    \"\"\"Calculate the score for the model using test data.\n\n    First, the model is fitted to the training data, and the time taken to\n    fit the model is recorded. Then, the model is scored using the provided\n    scoring method(s) on the `test` data.\n\n    In multi-output regression, by default, 'uniform_average' is used,\n    which specifies a uniformly weighted mean over outputs. see\n    https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics\n\n    For scoring paramers overview:\n    https://scikit-learn.org/stable/modules/model_evaluation.html#string-name-scorers\n\n    Args:\n        x_train (ArrayLike): training data for features\n        x_test (ArrayLike): test data for features\n        y_train (ArrayLike): training data for targets\n        y_test (ArrayLike): test data for targets\n        scoring (str | Iterable, optional): scoring method(s) to use.\n\n    Returns:\n        float | dict[str, float]: the calculated score(s)\n    \"\"\"  # noqa: E501\n    model_fit_time = _measure_fit_time(self.estimator, x_train, y_train)\n\n    # prepare scoring list and check if \"model_fit_time\" is included\n    scoring_list = [scoring] if isinstance(scoring, str) else list(scoring)\n    include_fit_time = \"model_fit_time\" in scoring_list\n    include_predict_time = \"model_predict_time\" in scoring_list\n\n    # Remove custom timing keys before passing to sklearn scorer\n    scoring_list = [\n        s for s in scoring_list if s not in {\"model_fit_time\", \"model_predict_time\"}\n    ]\n\n    if scoring_list:\n        scorer = check_scoring(self.estimator, scoring=scoring_list)\n        scores = scorer(self.estimator, x_test, y_test)\n    else:\n        scores = {}\n\n    if include_fit_time:\n        scores[\"model_fit_time\"] = model_fit_time\n\n    if include_predict_time:\n        scores[\"model_predict_time\"] = _measure_predict_latency(\n            self.estimator, x_test\n        )\n    return scores\n</code></pre>"},{"location":"api_reference/#fowt_ml.base.BaseModel.cross_validate","title":"cross_validate","text":"<pre><code>cross_validate(x_train: ArrayLike, y_train: ArrayLike, scoring: str | Iterable, **kwargs: Any) -&gt; dict[str, Any]\n</code></pre> <p>Perform cross-validation on the model.</p> <p>Parameters:</p> <ul> <li> <code>x_train</code>               (<code>ArrayLike</code>)           \u2013            <p>features data</p> </li> <li> <code>y_train</code>               (<code>ArrayLike</code>)           \u2013            <p>target data</p> </li> <li> <code>scoring</code>               (<code>str | Iterable</code>)           \u2013            <p>scoring method(s) to use.</p> </li> <li> <code>**kwargs</code>               (<code>Any</code>, default:                   <code>{}</code> )           \u2013            <p>additional keyword arguments to pass to <code>cross_validate</code></p> </li> </ul> <p>Returns:</p> <ul> <li> <code>dict[str, Any]</code>           \u2013            <p>dict[str, Any]: dictionary containing cross-validation results</p> </li> </ul> Source code in <code>src/fowt_ml/base.py</code> <pre><code>def cross_validate(\n    self,\n    x_train: ArrayLike,\n    y_train: ArrayLike,\n    scoring: str | Iterable,\n    **kwargs: Any,\n) -&gt; dict[str, Any]:\n    \"\"\"Perform cross-validation on the model.\n\n    Args:\n        x_train (ArrayLike): features data\n        y_train (ArrayLike): target data\n        scoring (str | Iterable, optional): scoring method(s) to use.\n        **kwargs: additional keyword arguments to pass to `cross_validate`\n\n    Returns:\n        dict[str, Any]: dictionary containing cross-validation results\n    \"\"\"\n    scoring_list = [scoring] if isinstance(scoring, str) else list(scoring)\n    include_fit_time = \"model_fit_time\" in scoring_list\n    include_predict_time = \"model_predict_time\" in scoring_list\n\n    scoring_list = [\n        s for s in scoring_list if s not in {\"model_fit_time\", \"model_predict_time\"}\n    ]\n\n    scorers = {}\n    if scoring_list:\n        scorers.update({s: s for s in scoring_list})\n    if include_predict_time:\n        scorers[\"model_predict_time\"] = _measure_predict_latency\n\n    cv_results = cross_validate(\n        self.estimator,\n        x_train,\n        y_train,\n        scoring=scorers or None,\n        return_train_score=False,\n        **kwargs,\n    )\n\n    results = {}\n    for k, v in cv_results.items():\n        if k.startswith(\"test_\"):\n            results[k.replace(\"test_\", \"\")] = v\n        elif include_fit_time and k == \"fit_time\":\n            results[\"model_fit_time\"] = np.round(v, 3)\n\n    return results\n</code></pre>"},{"location":"api_reference/#fowt_ml.base.BaseModel.use_scaled_data","title":"use_scaled_data","text":"<pre><code>use_scaled_data()\n</code></pre> <p>Wrap the estimator to use scaled data for both X and y.</p> Source code in <code>src/fowt_ml/base.py</code> <pre><code>def use_scaled_data(self):\n    \"\"\"Wrap the estimator to use scaled data for both X and y.\"\"\"\n    if isinstance(self.estimator, TransformedTargetRegressor):\n        return self  # already wrapped\n\n    # Pipeline for input scaling + model\n    regressor = Pipeline([(\"scaler\", StandardScaler()), (\"model\", self.estimator)])\n\n    # Wrap with TransformedTargetRegressor for y scaling\n    self.estimator = TransformedTargetRegressor(\n        regressor=regressor, transformer=StandardScaler()\n    )\n    return self\n</code></pre>"},{"location":"api_reference/#fowt_ml.linear_models","title":"fowt_ml.linear_models","text":"<p>Module to handle linear models.</p> <p>Classes:</p> <ul> <li> <code>LinearModels</code>           \u2013            <p>Class to handle linear models and metrics for comparison.</p> </li> </ul>"},{"location":"api_reference/#fowt_ml.linear_models.LinearModels","title":"LinearModels","text":"<pre><code>LinearModels(estimator: str | BaseEstimator, **kwargs: dict[str, Any])\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Class to handle linear models and metrics for comparison.</p> Source code in <code>src/fowt_ml/base.py</code> <pre><code>def __init__(\n    self, estimator: str | BaseEstimator, **kwargs: dict[str, Any]\n) -&gt; None:\n    \"\"\"Initialize the class with the estimator.\"\"\"\n    if isinstance(estimator, str):\n        if estimator not in self.ESTIMATOR_NAMES:\n            raise ValueError(f\"Available estimators: {self.ESTIMATOR_NAMES.keys()}\")\n        self.estimator = self.ESTIMATOR_NAMES[estimator](**kwargs)\n    else:\n        self.estimator = estimator.set_params(**kwargs)\n</code></pre>"},{"location":"api_reference/#fowt_ml.ensemble","title":"fowt_ml.ensemble","text":"<p>Class to handle random forest models and metrics for comparison.</p> <p>Classes:</p> <ul> <li> <code>EnsembleModel</code>           \u2013            <p>Class to handle random forest models and metrics for comparison.</p> </li> </ul>"},{"location":"api_reference/#fowt_ml.ensemble.EnsembleModel","title":"EnsembleModel","text":"<pre><code>EnsembleModel(estimator: str | BaseEstimator, **kwargs: dict[str, Any])\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Class to handle random forest models and metrics for comparison.</p> <p>Methods:</p> <ul> <li> <code>oob_score</code>             \u2013              <p>Fit and estimate generalization score from out-of-bag samples.</p> </li> </ul> Source code in <code>src/fowt_ml/base.py</code> <pre><code>def __init__(\n    self, estimator: str | BaseEstimator, **kwargs: dict[str, Any]\n) -&gt; None:\n    \"\"\"Initialize the class with the estimator.\"\"\"\n    if isinstance(estimator, str):\n        if estimator not in self.ESTIMATOR_NAMES:\n            raise ValueError(f\"Available estimators: {self.ESTIMATOR_NAMES.keys()}\")\n        self.estimator = self.ESTIMATOR_NAMES[estimator](**kwargs)\n    else:\n        self.estimator = estimator.set_params(**kwargs)\n</code></pre>"},{"location":"api_reference/#fowt_ml.ensemble.EnsembleModel.oob_score","title":"oob_score","text":"<pre><code>oob_score(x: ArrayLike, y: ArrayLike, scoring: str) -&gt; float\n</code></pre> <p>Fit and estimate generalization score from out-of-bag samples.</p> Source code in <code>src/fowt_ml/ensemble.py</code> <pre><code>def oob_score(self, x: ArrayLike, y: ArrayLike, scoring: str) -&gt; float:\n    \"\"\"Fit and estimate generalization score from out-of-bag samples.\"\"\"\n    scorer = get_scorer(scoring)\n\n    def score_func(y, y_pred, **kwargs):\n        return scorer._sign * scorer._score_func(y, y_pred, **kwargs)\n\n    oob_score = score_func\n    if not (self.estimator.bootstrap and self.estimator.oob_score):\n        warnings.warn(f\"Setting `bootstrap=True` and `oob_score={oob_score}`\")\n        self.estimator.set_params(bootstrap=True, oob_score=oob_score)\n    self.estimator.fit(x, y)\n    return self.estimator.oob_score_\n</code></pre>"},{"location":"api_reference/#fowt_ml.neural_network","title":"fowt_ml.neural_network","text":"<p>Module to handle Neural Network models.</p> <p>Classes:</p> <ul> <li> <code>GenericRNNModule</code>           \u2013            </li> <li> <code>NeuralNetwork</code>           \u2013            <p>Class to handle Neural Network models and metrics for comparison.</p> </li> </ul> <p>Functions:</p> <ul> <li> <code>create_skorch_regressor</code>             \u2013              <p>Create a skorch NeuralNetRegressor with a specified RNN model.</p> </li> <li> <code>RNNRegressor</code>             \u2013              <p>Create a skorch NeuralNetRegressor with a standard RNN model.</p> </li> <li> <code>LSTMRegressor</code>             \u2013              <p>Create a skorch NeuralNetRegressor with an LSTM model.</p> </li> <li> <code>GRURegressor</code>             \u2013              <p>Create a skorch NeuralNetRegressor with a GRU model.</p> </li> </ul>"},{"location":"api_reference/#fowt_ml.neural_network.GenericRNNModule","title":"GenericRNNModule","text":"<pre><code>GenericRNNModule(rnn_model, input_size, hidden_size, output_size, num_layers=1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Forward pass of the RNN module.</p> </li> </ul> Source code in <code>src/fowt_ml/neural_network.py</code> <pre><code>def __init__(self, rnn_model, input_size, hidden_size, output_size, num_layers=1):\n    super().__init__()\n    self.rnn = rnn_model(\n        input_size=input_size,\n        hidden_size=hidden_size,\n        num_layers=num_layers,\n        batch_first=True,\n    )\n    self.fc = torch.nn.Linear(hidden_size, output_size)\n</code></pre>"},{"location":"api_reference/#fowt_ml.neural_network.GenericRNNModule.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass of the RNN module.</p> Source code in <code>src/fowt_ml/neural_network.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward pass of the RNN module.\"\"\"\n    if x.dim() == 2:\n        x = x.unsqueeze(0)  # add batch dim\n    out, _ = self.rnn(x)\n\n    out_fc = self.fc(out)  # regression on all time steps\n    if out_fc.shape[0] == 1:\n        out_fc = out_fc.squeeze(0)\n    return out_fc\n</code></pre>"},{"location":"api_reference/#fowt_ml.neural_network.NeuralNetwork","title":"NeuralNetwork","text":"<pre><code>NeuralNetwork(estimator: str | BaseEstimator, **kwargs: dict[str, Any])\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Class to handle Neural Network models and metrics for comparison.</p> Source code in <code>src/fowt_ml/base.py</code> <pre><code>def __init__(\n    self, estimator: str | BaseEstimator, **kwargs: dict[str, Any]\n) -&gt; None:\n    \"\"\"Initialize the class with the estimator.\"\"\"\n    if isinstance(estimator, str):\n        if estimator not in self.ESTIMATOR_NAMES:\n            raise ValueError(f\"Available estimators: {self.ESTIMATOR_NAMES.keys()}\")\n        self.estimator = self.ESTIMATOR_NAMES[estimator](**kwargs)\n    else:\n        self.estimator = estimator.set_params(**kwargs)\n</code></pre>"},{"location":"api_reference/#fowt_ml.neural_network.create_skorch_regressor","title":"create_skorch_regressor","text":"<pre><code>create_skorch_regressor(rnn_model, input_size, hidden_size, output_size, num_layers=1, **kwargs)\n</code></pre> <p>Create a skorch NeuralNetRegressor with a specified RNN model.</p> Source code in <code>src/fowt_ml/neural_network.py</code> <pre><code>def create_skorch_regressor(\n    rnn_model,\n    input_size,\n    hidden_size,\n    output_size,\n    num_layers=1,\n    **kwargs,\n):\n    \"\"\"Create a skorch NeuralNetRegressor with a specified RNN model.\"\"\"\n    params = dict(\n        module=GenericRNNModule,\n        module__rnn_model=rnn_model,\n        module__input_size=input_size,\n        module__hidden_size=hidden_size,\n        module__output_size=output_size,\n        module__num_layers=num_layers,\n        verbose=0,\n    )\n    params.update(kwargs)\n    return skorch.regressor.NeuralNetRegressor(**params)\n</code></pre>"},{"location":"api_reference/#fowt_ml.neural_network.RNNRegressor","title":"RNNRegressor","text":"<pre><code>RNNRegressor(**args)\n</code></pre> <p>Create a skorch NeuralNetRegressor with a standard RNN model.</p> Source code in <code>src/fowt_ml/neural_network.py</code> <pre><code>def RNNRegressor(**args):  # noqa: N802\n    \"\"\"Create a skorch NeuralNetRegressor with a standard RNN model.\"\"\"\n    return create_skorch_regressor(torch.nn.RNN, **args)\n</code></pre>"},{"location":"api_reference/#fowt_ml.neural_network.LSTMRegressor","title":"LSTMRegressor","text":"<pre><code>LSTMRegressor(**args)\n</code></pre> <p>Create a skorch NeuralNetRegressor with an LSTM model.</p> Source code in <code>src/fowt_ml/neural_network.py</code> <pre><code>def LSTMRegressor(**args):  # noqa: N802\n    \"\"\"Create a skorch NeuralNetRegressor with an LSTM model.\"\"\"\n    return create_skorch_regressor(torch.nn.LSTM, **args)\n</code></pre>"},{"location":"api_reference/#fowt_ml.neural_network.GRURegressor","title":"GRURegressor","text":"<pre><code>GRURegressor(**args)\n</code></pre> <p>Create a skorch NeuralNetRegressor with a GRU model.</p> Source code in <code>src/fowt_ml/neural_network.py</code> <pre><code>def GRURegressor(**args):  # noqa: N802\n    \"\"\"Create a skorch NeuralNetRegressor with a GRU model.\"\"\"\n    return create_skorch_regressor(torch.nn.GRU, **args)\n</code></pre>"},{"location":"api_reference/#fowt_ml.gaussian_process","title":"fowt_ml.gaussian_process","text":"<p>Module for sparse Gaussian process for multi-output regeression problem.</p> <p>Classes:</p> <ul> <li> <code>MultitaskGPModelApproximate</code>           \u2013            <p>Multitask GP model with approximate inference.</p> </li> <li> <code>SklearnGPRegressor</code>           \u2013            <p>Sklearn Wrapper for MultitaskGPModelApproximate.</p> </li> <li> <code>SparseGaussianModel</code>           \u2013            <p>Class to handle sparse Gaussian process regression.</p> </li> </ul>"},{"location":"api_reference/#fowt_ml.gaussian_process.MultitaskGPModelApproximate","title":"MultitaskGPModelApproximate","text":"<pre><code>MultitaskGPModelApproximate(inducing_points, num_latents, num_tasks)\n</code></pre> <p>               Bases: <code>ApproximateGP</code></p> <p>Multitask GP model with approximate inference.</p> <p>This module models similarities/correlation in the outputs simultaneously. Each output dimension (task) is the linear combination of some latent function. Base on example https://docs.gpytorch.ai/en/stable/examples/04_Variational_and_Approximate_GPs/SVGP_Multitask_GP_Regression.html#Types-of-Variational-Multitask-Models</p> <p>Methods:</p> <ul> <li> <code>forward</code>             \u2013              <p>Forward pass of the model.</p> </li> </ul> Source code in <code>src/fowt_ml/gaussian_process.py</code> <pre><code>def __init__(self, inducing_points, num_latents, num_tasks):\n    # convert inducing points to tensor\n    inducing_points = _to_tensor(inducing_points, dtype=\"float32\", device=DEVICE)\n\n    # Variational distribution + strategy: posterior for latent GPs\n    # CholeskyVariationalDistribution: modeling a full covariance (not\n    # diagonal), so it can capture dependencies between inducing points\n    variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n        num_inducing_points=inducing_points.size(0),\n        batch_shape=torch.Size([num_latents]),\n    )\n\n    # Check inducing points shape before passing to VariationalStrategy\n    inducing_points = _check_inducing_points(inducing_points, num_latents)\n\n    # model correlations across tasks (or outputs)\n    variational_strategy = gpytorch.variational.LMCVariationalStrategy(\n        gpytorch.variational.VariationalStrategy(\n            self,\n            inducing_points,\n            variational_distribution,\n            learn_inducing_locations=True,\n        ),\n        num_tasks=num_tasks,\n        num_latents=num_latents,\n        latent_dim=-1,\n    )\n    super().__init__(variational_strategy)\n\n    # covariance module: kernel: Prior information about latents\n    self.covar = gpytorch.kernels.ScaleKernel(\n        gpytorch.kernels.RBFKernel(batch_shape=torch.Size([num_latents])),\n        batch_shape=torch.Size([num_latents]),\n    )\n    # Mean module\n    self.mean = gpytorch.means.ConstantMean(\n        batch_shape=torch.Size([num_latents]),\n    )\n\n    self.likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(\n        num_tasks=num_tasks\n    ).to(DEVICE)\n</code></pre>"},{"location":"api_reference/#fowt_ml.gaussian_process.MultitaskGPModelApproximate.forward","title":"forward","text":"<pre><code>forward(x)\n</code></pre> <p>Forward pass of the model.</p> Source code in <code>src/fowt_ml/gaussian_process.py</code> <pre><code>def forward(self, x):\n    \"\"\"Forward pass of the model.\"\"\"\n    mean_x = self.mean(x)\n    covar_x = self.covar(x)\n    return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n</code></pre>"},{"location":"api_reference/#fowt_ml.gaussian_process.SklearnGPRegressor","title":"SklearnGPRegressor","text":"<pre><code>SklearnGPRegressor(num_inducing, num_latents, num_epochs=10, batch_size=1024, learning_rate=0.01)\n</code></pre> <p>               Bases: <code>RegressorMixin</code>, <code>BaseEstimator</code></p> <p>Sklearn Wrapper for MultitaskGPModelApproximate.</p> <p>Methods:</p> <ul> <li> <code>fit</code>             \u2013              <p>Fit the model to the training data.</p> </li> <li> <code>predict</code>             \u2013              <p>Make predictions using the trained model.</p> </li> <li> <code>score</code>             \u2013              <p>Return the R^2 score of the prediction.</p> </li> </ul> Source code in <code>src/fowt_ml/gaussian_process.py</code> <pre><code>def __init__(\n    self,\n    num_inducing,\n    num_latents,\n    num_epochs=10,\n    batch_size=1024,\n    learning_rate=0.01,\n):\n    self.num_inducing = num_inducing\n    self.num_latents = num_latents\n    self.num_epochs = num_epochs\n    self.batch_size = batch_size\n    self.learning_rate = learning_rate\n</code></pre>"},{"location":"api_reference/#fowt_ml.gaussian_process.SklearnGPRegressor.fit","title":"fit","text":"<pre><code>fit(x_train: ArrayLike, y_train: ArrayLike) -&gt; SklearnGPRegressor\n</code></pre> <p>Fit the model to the training data.</p> Source code in <code>src/fowt_ml/gaussian_process.py</code> <pre><code>def fit(self, x_train: ArrayLike, y_train: ArrayLike) -&gt; \"SklearnGPRegressor\":\n    \"\"\"Fit the model to the training data.\"\"\"\n    # Check that X and y have correct shape\n    x_train, y_train = check_X_y(x_train, y_train, multi_output=True)\n\n    x_train = _to_tensor(x_train, dtype=\"float32\", device=DEVICE)\n    y_train = _to_tensor(y_train, dtype=\"float32\", device=DEVICE)\n\n    # add some sklearn variables\n    self.X_ = x_train\n    self.y_ = y_train\n    self.n_features_in_ = x_train.shape[1]\n\n    # initialize model\n    if y_train.ndim == 1:\n        y_train = y_train.unsqueeze(1)\n\n    inducing_points = x_train[torch.randperm(x_train.size(0))[: self.num_inducing]]\n\n    self.module_ = MultitaskGPModelApproximate(\n        inducing_points=inducing_points,\n        num_latents=self.num_latents,\n        num_tasks=y_train.size(1),\n    ).to(DEVICE)\n\n    self.likelihood_ = self.module_.likelihood\n\n    # Train the model\n    self.module_.train()\n    self.likelihood_.train()\n\n    optimizer = torch.optim.Adam(self.module_.parameters(), lr=self.learning_rate)\n    # marginal log likelihood (mll)\n    mll = gpytorch.mlls.VariationalELBO(\n        self.likelihood_, self.module_, num_data=x_train.size(0)\n    )\n\n    # TODO optimize the loops\n    for epoch in range(self.num_epochs):\n        total_loss = 0\n        if self.batch_size:  # Use batching if batch_size is set\n            batches = DataLoader(\n                TensorDataset(x_train, y_train),\n                batch_size=self.batch_size,\n            )\n        else:  # Treat entire dataset as one batch\n            batches = [(x_train, y_train)]\n        for x_batch, y_batch in batches:\n            optimizer.zero_grad()\n            output = self.module_(x_batch)\n            loss = -mll(output, y_batch)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss\n\n        # normalize the loss per data point and output dimension because it\n        # gives a better idea of the loss in log\n        ave_loss = total_loss.item() / (x_train.size(0) * y_train.size(1))\n        logger.info(f\"Epoch {epoch + 1}/{self.num_epochs} - Loss: {ave_loss:.3f}\")\n\n    self.is_fitted_ = True\n    return self\n</code></pre>"},{"location":"api_reference/#fowt_ml.gaussian_process.SklearnGPRegressor.predict","title":"predict","text":"<pre><code>predict(x_array: ArrayLike) -&gt; ArrayLike\n</code></pre> <p>Make predictions using the trained model.</p> Source code in <code>src/fowt_ml/gaussian_process.py</code> <pre><code>def predict(self, x_array: ArrayLike) -&gt; ArrayLike:\n    \"\"\"Make predictions using the trained model.\"\"\"\n    # Check if the model has been fitted\n    check_is_fitted(self, [\"is_fitted_\", \"module_\", \"likelihood_\"])\n\n    # Check that X has correct shape\n    x_array = check_array(x_array)\n\n    # Check number of features\n    if x_array.shape[1] != self.n_features_in_:\n        raise ValueError(\n            f\"Expected {self.n_features_in_} features, \"\n            f\"but got {x_array.shape[1]} features.\"\n        )\n    x_array = _to_tensor(x_array, dtype=\"float32\", device=DEVICE)\n\n    self.module_.eval()\n    self.likelihood_.eval()\n\n    all_preds = []\n    with torch.no_grad():\n        if self.batch_size:  # Use batching if batch_size is set\n            batches = DataLoader(\n                TensorDataset(x_array),\n                batch_size=self.batch_size,\n            )\n        else:  # Treat entire dataset as one batch\n            batches = [(x_array,)]\n        for (x_batch,) in batches:\n            predictions = self.likelihood_(self.module_(x_batch))\n            all_preds.append(predictions.mean.cpu())\n\n    # sklearn multioutput regressor expects float64\n    # see check_multioutput_regressor\n    return torch.cat(all_preds, dim=0).numpy().astype(np.float64)\n</code></pre>"},{"location":"api_reference/#fowt_ml.gaussian_process.SklearnGPRegressor.score","title":"score","text":"<pre><code>score(x, y)\n</code></pre> <p>Return the R^2 score of the prediction.</p> Source code in <code>src/fowt_ml/gaussian_process.py</code> <pre><code>def score(self, x, y):\n    \"\"\"Return the R^2 score of the prediction.\"\"\"\n    y_pred = self.predict(x)\n    return r2_score(y, y_pred)\n</code></pre>"},{"location":"api_reference/#fowt_ml.gaussian_process.SparseGaussianModel","title":"SparseGaussianModel","text":"<pre><code>SparseGaussianModel(estimator: str | BaseEstimator, **kwargs: dict[str, Any])\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Class to handle sparse Gaussian process regression.</p> Source code in <code>src/fowt_ml/base.py</code> <pre><code>def __init__(\n    self, estimator: str | BaseEstimator, **kwargs: dict[str, Any]\n) -&gt; None:\n    \"\"\"Initialize the class with the estimator.\"\"\"\n    if isinstance(estimator, str):\n        if estimator not in self.ESTIMATOR_NAMES:\n            raise ValueError(f\"Available estimators: {self.ESTIMATOR_NAMES.keys()}\")\n        self.estimator = self.ESTIMATOR_NAMES[estimator](**kwargs)\n    else:\n        self.estimator = estimator.set_params(**kwargs)\n</code></pre>"},{"location":"api_reference/#fowt_ml.xgboost","title":"fowt_ml.xgboost","text":"<p>The module for XGBoost model training and evaluation.</p> <p>Classes:</p> <ul> <li> <code>XGBoost</code>           \u2013            <p>Class to handle linear models and metrics for comparison.</p> </li> </ul>"},{"location":"api_reference/#fowt_ml.xgboost.XGBoost","title":"XGBoost","text":"<pre><code>XGBoost(estimator: str | BaseEstimator, **kwargs: dict[str, Any])\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Class to handle linear models and metrics for comparison.</p> Source code in <code>src/fowt_ml/base.py</code> <pre><code>def __init__(\n    self, estimator: str | BaseEstimator, **kwargs: dict[str, Any]\n) -&gt; None:\n    \"\"\"Initialize the class with the estimator.\"\"\"\n    if isinstance(estimator, str):\n        if estimator not in self.ESTIMATOR_NAMES:\n            raise ValueError(f\"Available estimators: {self.ESTIMATOR_NAMES.keys()}\")\n        self.estimator = self.ESTIMATOR_NAMES[estimator](**kwargs)\n    else:\n        self.estimator = estimator.set_params(**kwargs)\n</code></pre>"},{"location":"developer_guide/","title":"Developer guide","text":"<p>If you are a developer wanting to contribute to the <code>FOWT-ML</code> package, this guide will help you get started. First check out the contribution guidelines in Contributing guide and the Project setup to get familiar with the package structure and development practices.</p>"},{"location":"developer_guide/#installation-in-development-mode","title":"Installation in development mode","text":"<p>To install the package in development mode, you need to clone the source code and install the package in development mode:</p> <pre><code>git clone git@github.com:hybridlabs-nl/fowt-ml.git\ncd FOWT-ML\npip install -e .[dev]  ## install with development dependencies\npip install -e .[docs]  ## install with documentation dependencies\n</code></pre>"},{"location":"developer_guide/#github-collaboration-workflow","title":"GitHub collaboration workflow","text":"<p>We use a GitHub collaboration workflow based on feature branches and pull requests. When starting adding a new feature or fixing a bug, create a new branch from <code>main</code> branch. When your changes are ready, create a pull request to merge your changes back into <code>main</code> branch. Make sure to ask for at least one review from another team member before merging your pull request.</p>"},{"location":"developer_guide/#running-the-tests","title":"Running the tests","text":"<ul> <li>Tests should be put in the <code>tests</code> folder.</li> <li>The testing framework used is PyTest</li> <li>The project uses GitHub action workflows   to automatically run tests on GitHub infrastructure against multiple Python   versions. Workflows can be found in <code>.github/workflows</code> directory.</li> <li>Relevant section in the   guide</li> <li>To run the tests locally, you need to make sure that you have installed the development dependencies as described in the Installation in development mode section. Then, inside the package directory, run:</li> </ul> <pre><code>pytest -v\n</code></pre> <p>to run all tests with verbose output. To run an individual test file, run:</p> <pre><code>pytest -v tests/test_my_module.py\n</code></pre>"},{"location":"developer_guide/#linters","title":"Linters","text":"<p>For linting and sorting imports we will use ruff. Running the linters requires an activated virtual environment with the development tools installed.</p> <pre><code># linter\nruff check .\n\n# linter with automatic fixing\nruff check . --fix\n\n# check formatting only\nruff format --check . --diff\n</code></pre>"},{"location":"developer_guide/#documentation-page","title":"Documentation page","text":"<ul> <li>Documentation should be put in the <code>docs/</code> directory.</li> <li>We recommend writing the documentation using Google style docstrings.</li> <li>The documentation is set up with the MkDocs.</li> <li><code>.mkdocs.yml</code> is the MkDocs configuration file. When MkDocs is building the documentation this package and its development dependencies are installed so the API reference can be rendered.</li> <li>Make sure you have installed the documentation dependencies as described in the Installation in development mode section. Then, inside the package directory, run:</li> </ul> <pre><code># Build the documentation\nmkdocs build\n\n# Preview the documentation\nmkdocs serve\n</code></pre> <p>Click on the link provided in the terminal to view the documentation page.</p>"},{"location":"developer_guide/#coding-style-conventions-and-code-quality","title":"Coding style conventions and code quality","text":"<ul> <li>Relevant section in the NLeSC guide.</li> </ul>"},{"location":"developer_guide/#continuous-code-quality","title":"Continuous code quality","text":"<p>Sonarcloud is used to perform quality analysis and code coverage report</p> <ul> <li><code>sonar-project.properties</code> is the SonarCloud configuration file</li> <li><code>.github/workflows/sonarcloud.yml</code> is the GitHub action workflow which performs the SonarCloud analysis.</li> </ul>"},{"location":"examples/","title":"Example notebooks","text":""},{"location":"examples/#configuraion-file","title":"Configuraion file","text":"<p>In FOWT-ML, you can use a configuration file to set up your experiment. The configuration file is typically written in YAML format and allows you to specify various settings such as data path, model parameters, and simulation options.This allows you to share and reproduce experiments easily.</p> <p>An example configuration file is available at [FOWT-ML/src/example_config.yml] (https://github.com/hybridlabs-nl/FOWT-ML/blob/main/src/example_config.yml). See the example notebooks on how to use it.</p>"},{"location":"examples/#training-a-model","title":"Training a model","text":"<p>We provide an example notebook showing how to train and test a model using FOWT-ML. To get familiar with the concepts and implementation, check out <code>Introduction</code> in the documentation.</p> Access the source View the notebook"},{"location":"examples/#comparing-different-models","title":"Comparing different models","text":"<p>We provide an example notebook showing how to compare different models using FOWT-ML pipline.</p> Access the source View the notebook"},{"location":"installation/","title":"Installation","text":"<p>To install <code>FOWT-ML</code> from GitHub repository, do:</p> <pre><code>git clone git@github.com:hybridlabs-nl/fowt-ml.git\ncd FOWT-ML\npython -m pip install .\n</code></pre> <p>To install the package in development mode, you can clone the repository and install it using pip:</p> <pre><code>pip install -e .[dev]\n</code></pre> <p>To work with notebooks, you need to install <code>jupyterlab</code>:</p> <pre><code>pip install jupyterlab\n</code></pre> <p>If you are a contributor, follow the instructions in the <code>How to Contribute</code> in the documentation.</p>"},{"location":"project_setup/","title":"Project setup","text":"<p>Here we provide some details about the project setup. Most of the choices are explained in the Turing Way: Guide for Reproducible Research.</p>"},{"location":"project_setup/#repository-structure","title":"Repository structure","text":"<p>The repository has the following structure:</p> <pre><code>\u251c\u2500\u2500 .github/        # GitHub specific files such as workflows\n\u251c\u2500\u2500 docs/           # Documentation source files\n\u251c\u2500\u2500 src/fowt_ml     # Main package code\n\u251c\u2500\u2500 tests/          # Test code\n\u251c\u2500\u2500 .gitignore      # Git ignore file\n\u251c\u2500\u2500 CITATION.cff    # Citation file\n\u251c\u2500\u2500 LICENSE         # License file\n\u251c\u2500\u2500 README.md       # User documentation\n\u251c\u2500\u2500 pyproject.toml  # Project configuration file and dependencies\n\u251c\u2500\u2500 mkdocs.yml      # MkDocs configuration file\n</code></pre>"},{"location":"project_setup/#package-management-and-dependencies","title":"Package management and dependencies","text":"<p>You can use pip for installing dependencies and package management.</p> <ul> <li>Runtime dependencies should be added to <code>pyproject.toml</code> in the <code>dependencies</code>   list under <code>[project]</code>.</li> <li>Development dependencies, such as for testing or documentation, should be   added to <code>pyproject.toml</code> in one of the lists under   <code>[project.optional-dependencies]</code>.</li> </ul>"},{"location":"project_setup/#packagingone-command-install","title":"Packaging/One command install","text":"<p>You can distribute your code using PyPI. This can be done automatically using GitHub workflows, see <code>.github/</code>.</p>"},{"location":"project_setup/#package-version-number","title":"Package version number","text":"<ul> <li>We recommend using semantic versioning.</li> <li>For convenience, the package version is stored in a single place: <code>pyproject.toml</code>.</li> <li>Don't forget to update the version number before making a release! Also, update <code>__version__</code> variable in <code>src/fowt_ml/__init__.py</code> to the    same version.</li> </ul>"},{"location":"project_setup/#citationcff","title":"CITATION.cff","text":"<ul> <li>To allow others to cite your software, add a <code>CITATION.cff</code> file</li> <li>It only makes sense to do this once there is something to cite (e.g., a software release with a DOI).</li> <li>Follow the Software Citation with CITATION.cff section in the Turing Way guide.</li> </ul>"},{"location":"project_setup/#code_of_conductmd","title":"CODE_OF_CONDUCT.md","text":"<ul> <li>Information about how to behave professionally</li> <li>To know more, read Turing Way guide on Code of Conduct</li> </ul>"},{"location":"project_setup/#contributingmd","title":"CONTRIBUTING.md","text":"<ul> <li>Information about how to contribute to this software package</li> <li>To know more, read Turing Way guide on Contributing</li> </ul>"},{"location":"notebooks/compare_models/","title":"Compare models in a hybrid setup: Motion feedback- Force actuation","text":"In\u00a0[1]: Copied! <pre>from fowt_ml.pipeline import Pipeline\n</pre> from fowt_ml.pipeline import Pipeline In\u00a0[2]: Copied! <pre>example_config_file = \"../../src/example_config.yml\"\n</pre> example_config_file = \"../../src/example_config.yml\" In\u00a0[3]: Copied! <pre>my_pipeline = Pipeline(example_config_file)\n</pre> my_pipeline = Pipeline(example_config_file) In\u00a0[4]: Copied! <pre># set correct path for mat file: add the correct path\nmy_pipeline.data_config[\"exp699\"][\"path_file\"] = \"/home/sarah/temp/hybridlabs/data_example/exp699.mat\"\n</pre> # set correct path for mat file: add the correct path my_pipeline.data_config[\"exp699\"][\"path_file\"] = \"/home/sarah/temp/hybridlabs/data_example/exp699.mat\" In\u00a0[5]: Copied! <pre># check the targets and predictors that are provided in the comfig\nprint(\"targets are:\", my_pipeline.target_labels)\n</pre> # check the targets and predictors that are provided in the comfig print(\"targets are:\", my_pipeline.target_labels) <pre>targets are: ['force_tt_meas6[0]', 'force_tt_meas6[1]', 'force_tt_meas6[2]', 'force_tt_meas6[3]', 'force_tt_meas6[4]', 'force_tt_meas6[5]']\n</pre> In\u00a0[6]: Copied! <pre>print(\"features are:\", my_pipeline.predictors_labels)\n</pre> print(\"features are:\", my_pipeline.predictors_labels) <pre>features are: ['pos_act6[0]', 'pos_act6[1]', 'pos_act6[2]', 'pos_act6[3]', 'pos_act6[4]', 'pos_act6[5]', 'spd_rot_act', 'wind_speed']\n</pre> In\u00a0[7]: Copied! <pre># check the models and metrics that are provided by the config\nprint(my_pipeline.model_names)\n</pre> # check the models and metrics that are provided by the config print(my_pipeline.model_names) <pre>{'ElasticNetRegression': {}, 'LeastAngleRegression': {}, 'LassoRegression': {}, 'LinearRegression': {}, 'SklearnGPRegressor': {'num_inducing': 100, 'num_latents': 3, 'num_epochs': 10}, 'RandomForest': {'n_estimators': 50, 'max_depth': 5, 'bootstrap': True, 'max_samples': 10000}, 'MultilayerPerceptron': {'hidden_layer_sizes': 10, 'max_iter': 50}, 'XGBoostRegression': {'n_estimators': 10, 'max_depth': 10, 'tree_method': 'hist'}, 'RNNRegressor': {'input_size': 14, 'hidden_size': 64, 'num_layers': 2, 'output_size': 6, 'max_epochs': 5}, 'LSTMRegressor': {'input_size': 14, 'hidden_size': 64, 'num_layers': 2, 'output_size': 6, 'max_epochs': 5}, 'GRURegressor': {'input_size': 14, 'hidden_size': 64, 'num_layers': 2, 'output_size': 6, 'max_epochs': 5}}\n</pre> In\u00a0[8]: Copied! <pre>print(my_pipeline.metric_names)\n</pre> print(my_pipeline.metric_names) <pre>['neg_mean_absolute_error', 'neg_root_mean_squared_error', 'r2', 'model_fit_time', 'model_predict_time']\n</pre> In\u00a0[9]: Copied! <pre># setup the pipeline\nmy_pipeline.setup(data=\"exp699\")\n</pre> # setup the pipeline my_pipeline.setup(data=\"exp699\") In\u00a0[10]: Copied! <pre># compare models on test data\nmodels, scores = my_pipeline.compare_models(sort=\"model_fit_time\")\n</pre> # compare models on test data models, scores = my_pipeline.compare_models(sort=\"model_fit_time\") <pre>2025/11/07 16:36:10 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2025-04-15; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'fowt-ml'}\n2025/11/07 16:36:23 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2025-04-15; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'fowt-ml'}\n2025/11/07 16:36:28 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2025-04-15; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'fowt-ml'}\n2025/11/07 16:36:32 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2025-04-15; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'fowt-ml'}\nINFO:fowt_ml.pipeline:Saving grid scores to grid_scores.csv\nINFO:fowt_ml.pipeline:Saving best model to joblib format in best_model.joblib\n</pre> In\u00a0[11]: Copied! <pre>scores\n</pre> scores Out[11]: neg_mean_absolute_error neg_root_mean_squared_error r2 model_fit_time model_predict_time LassoRegression -1.634784 -2.166301 -0.000028 0.180 0.000 LinearRegression -1.603058 -2.130906 0.107320 0.197 0.000 ElasticNetRegression -1.633379 -2.164612 0.010868 0.213 0.002 LeastAngleRegression -1.603058 -2.130906 0.107320 0.214 0.000 XGBoostRegression -1.581626 -2.086574 0.153210 3.239 0.001 RandomForest -1.600439 -2.122963 0.106379 4.171 0.002 MultilayerPerceptron -1.597414 -2.118111 0.120811 7.175 0.001 LSTMRegressor -1.625406 -2.155499 0.033197 44.764 0.001 SklearnGPRegressor -1.597856 -2.121320 0.124109 70.427 0.003 RNNRegressor -1.601624 -2.128001 0.110522 171.203 0.001 GRURegressor -1.606470 -2.133881 0.090896 473.594 0.001 In\u00a0[12]: Copied! <pre>models\n</pre> models Out[12]: <pre>{'ElasticNetRegression': TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n                                                       StandardScaler()),\n                                                      ('model', ElasticNet())]),\n                            transformer=StandardScaler()),\n 'LeastAngleRegression': TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n                                                       StandardScaler()),\n                                                      ('model', Lars())]),\n                            transformer=StandardScaler()),\n 'LassoRegression': TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n                                                       StandardScaler()),\n                                                      ('model', Lasso())]),\n                            transformer=StandardScaler()),\n 'LinearRegression': TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n                                                       StandardScaler()),\n                                                      ('model',\n                                                       LinearRegression())]),\n                            transformer=StandardScaler()),\n 'SklearnGPRegressor': TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n                                                       StandardScaler()),\n                                                      ('model',\n                                                       SklearnGPRegressor(num_inducing=100,\n                                                                          num_latents=3))]),\n                            transformer=StandardScaler()),\n 'RandomForest': TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n                                                       StandardScaler()),\n                                                      ('model',\n                                                       RandomForestRegressor(max_depth=5,\n                                                                             max_samples=10000,\n                                                                             n_estimators=50))]),\n                            transformer=StandardScaler()),\n 'MultilayerPerceptron': TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n                                                       StandardScaler()),\n                                                      ('model',\n                                                       MLPRegressor(hidden_layer_sizes=10,\n                                                                    max_iter=50))]),\n                            transformer=StandardScaler()),\n 'XGBoostRegression': TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n                                                       StandardScaler()),\n                                                      ('model',\n                                                       XGBRegressor(base_score=None,\n                                                                    booster=None,\n                                                                    callbacks=None,\n                                                                    colsample_bylevel=None,\n                                                                    colsample_bynode=None,\n                                                                    colsample_bytree=None,\n                                                                    device=None,\n                                                                    early_stopping_rounds=None,\n                                                                    enable_categorical=False,\n                                                                    eval_metric=None,\n                                                                    feature_types=None,\n                                                                    feature_weights=None,\n                                                                    gamma=None,\n                                                                    grow...None,\n                                                                    importance_type=None,\n                                                                    interaction_constraints=None,\n                                                                    learning_rate=None,\n                                                                    max_bin=None,\n                                                                    max_cat_threshold=None,\n                                                                    max_cat_to_onehot=None,\n                                                                    max_delta_step=None,\n                                                                    max_depth=10,\n                                                                    max_leaves=None,\n                                                                    min_child_weight=None,\n                                                                    missing=nan,\n                                                                    monotone_constraints=None,\n                                                                    multi_strategy=None,\n                                                                    n_estimators=10,\n                                                                    n_jobs=None,\n                                                                    num_parallel_tree=None, ...))]),\n                            transformer=StandardScaler()),\n 'RNNRegressor': TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n                                                       StandardScaler()),\n                                                      ('model',\n                                                       NeuralNetRegressor(_params_to_validate={'module__num_layers', 'module__hidden_size', 'module__rnn_model', 'module__input_size', 'module__output_size'}, batch_size=128, callbacks=None, compile=False, dataset=&lt;class 'skorch.dataset.Dataset'&gt;, device='cpu', iterator_train=&lt;...4, module__input_size=14, module__num_layers=2, module__output_size=6, module__rnn_model=&lt;class 'torch.nn.modules.rnn.RNN'&gt;, optimizer=&lt;class 'torch.optim.sgd.SGD'&gt;, predict_nonlinearity='auto', torch_load_kwargs=None, train_split=&lt;skorch.dataset.ValidSplit object at 0x7607b1ebc7d0&gt;, use_caching='auto', verbose=0, warm_start=False))]),\n                            transformer=StandardScaler()),\n 'LSTMRegressor': TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n                                                       StandardScaler()),\n                                                      ('model',\n                                                       NeuralNetRegressor(_params_to_validate={'module__num_layers', 'module__hidden_size', 'module__rnn_model', 'module__input_size', 'module__output_size'}, batch_size=128, callbacks=None, compile=False, dataset=&lt;class 'skorch.dataset.Dataset'&gt;, device='cpu', iterator_train=&lt;..., module__input_size=14, module__num_layers=2, module__output_size=6, module__rnn_model=&lt;class 'torch.nn.modules.rnn.LSTM'&gt;, optimizer=&lt;class 'torch.optim.sgd.SGD'&gt;, predict_nonlinearity='auto', torch_load_kwargs=None, train_split=&lt;skorch.dataset.ValidSplit object at 0x7607b1ebc7d0&gt;, use_caching='auto', verbose=0, warm_start=False))]),\n                            transformer=StandardScaler()),\n 'GRURegressor': TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n                                                       StandardScaler()),\n                                                      ('model',\n                                                       NeuralNetRegressor(_params_to_validate={'module__num_layers', 'module__hidden_size', 'module__rnn_model', 'module__input_size', 'module__output_size'}, batch_size=128, callbacks=None, compile=False, dataset=&lt;class 'skorch.dataset.Dataset'&gt;, device='cpu', iterator_train=&lt;...4, module__input_size=14, module__num_layers=2, module__output_size=6, module__rnn_model=&lt;class 'torch.nn.modules.rnn.GRU'&gt;, optimizer=&lt;class 'torch.optim.sgd.SGD'&gt;, predict_nonlinearity='auto', torch_load_kwargs=None, train_split=&lt;skorch.dataset.ValidSplit object at 0x7607b1ebc7d0&gt;, use_caching='auto', verbose=0, warm_start=False))]),\n                            transformer=StandardScaler())}</pre> In\u00a0[13]: Copied! <pre># compare models using cross_validation\nmodels, scores = my_pipeline.compare_models(sort=\"model_predict_time\", cross_validation=True)\n</pre> # compare models using cross_validation models, scores = my_pipeline.compare_models(sort=\"model_predict_time\", cross_validation=True) <pre>INFO:fowt_ml.pipeline:Logging experiment to MLflow with id 542351623230069078\n2025/11/07 17:20:46 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2025-04-15; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'fowt-ml'}\n2025/11/07 17:20:59 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2025-04-15; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'fowt-ml'}\n2025/11/07 17:21:04 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2025-04-15; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'fowt-ml'}\n2025/11/07 17:21:08 WARNING mlflow.utils.requirements_utils: The following packages were not found in the public PyPI package index as of 2025-04-15; if these packages are not present in the public PyPI index, you must install them manually before loading your model: {'fowt-ml'}\nINFO:fowt_ml.pipeline:Saving grid scores to grid_scores.csv\nINFO:fowt_ml.pipeline:Saving best model to joblib format in best_model.joblib\n</pre> In\u00a0[14]: Copied! <pre>scores\n</pre> scores Out[14]: model_fit_time neg_mean_absolute_error neg_root_mean_squared_error r2 model_predict_time MultilayerPerceptron 6.433333 -1.601874 -2.126851 0.120934 0.000667 LeastAngleRegression 0.190333 -1.607107 -2.138802 0.108088 0.001000 XGBoostRegression 2.320000 -1.590403 -2.102121 0.146356 0.001000 LinearRegression 0.214333 -1.607107 -2.138802 0.108088 0.001000 LSTMRegressor 30.154333 -1.633139 -2.168106 0.018256 0.001000 GRURegressor 358.188667 -1.615096 -2.147482 0.078020 0.001000 RNNRegressor 115.061000 -1.606321 -2.137074 0.110053 0.001000 LassoRegression 0.185000 -1.638445 -2.174113 -0.000020 0.001333 ElasticNetRegression 0.168000 -1.637037 -2.172398 0.011023 0.001667 RandomForest 3.776667 -1.604963 -2.131216 0.107301 0.002000 SklearnGPRegressor 51.833667 -1.602236 -2.130439 0.122882 0.003000 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/compare_models/#compare-models-in-a-hybrid-setup-motion-feedback-force-actuation","title":"Compare models in a hybrid setup: Motion feedback- Force actuation\u00b6","text":"<ul> <li>numerical model: simulate the floater dynamics.</li> <li>physical turbine: measure forces and accelerations</li> <li>Ml model: multi-output regressor</li> </ul>"},{"location":"notebooks/examples_models/","title":"How to run a model using and calculate scores","text":"In\u00a0[3]: Copied! <pre>from fowt_ml.datasets import get_data, fix_column_names\nfrom fowt_ml import Config\nfrom sklearn.model_selection import train_test_split\n</pre> from fowt_ml.datasets import get_data, fix_column_names from fowt_ml import Config from sklearn.model_selection import train_test_split In\u00a0[4]: Copied! <pre>example_config_file = \"../../src/example_config.yml\"\n</pre> example_config_file = \"../../src/example_config.yml\" In\u00a0[5]: Copied! <pre>config = Config.from_yaml(example_config_file)\nconfig[\"data\"][\"exp699\"][\"path_file\"] = \"/home/sarah/temp/hybridlabs/data_example/exp699.mat\"\ndf = get_data(\"exp699\", config[\"data\"])\n</pre> config = Config.from_yaml(example_config_file) config[\"data\"][\"exp699\"][\"path_file\"] = \"/home/sarah/temp/hybridlabs/data_example/exp699.mat\" df = get_data(\"exp699\", config[\"data\"]) In\u00a0[6]: Copied! <pre># train/test split\npredictors_labels = config[\"ml_setup\"][\"predictors\"]\ntarget_labels = config[\"ml_setup\"][\"targets\"]\n\n# rename the column names to exclude []\nX_data = df.loc[:, predictors_labels]\nY_data = df.loc[:, target_labels]\n\nX_data, Y_data = fix_column_names(X_data), fix_column_names(Y_data)\ntrain_test_split_kwargs = config[\"ml_setup\"][\"train_test_split_kwargs\"]\nX_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, **train_test_split_kwargs)\n</pre> # train/test split predictors_labels = config[\"ml_setup\"][\"predictors\"] target_labels = config[\"ml_setup\"][\"targets\"]  # rename the column names to exclude [] X_data = df.loc[:, predictors_labels] Y_data = df.loc[:, target_labels]  X_data, Y_data = fix_column_names(X_data), fix_column_names(Y_data) train_test_split_kwargs = config[\"ml_setup\"][\"train_test_split_kwargs\"] X_train, X_test, y_train, y_test = train_test_split(X_data, Y_data, **train_test_split_kwargs) In\u00a0[7]: Copied! <pre>metrics = config[\"ml_setup\"][\"metric_names\"]\nmetrics\n</pre> metrics = config[\"ml_setup\"][\"metric_names\"] metrics Out[7]: <pre>['neg_mean_absolute_error',\n 'neg_root_mean_squared_error',\n 'r2',\n 'model_fit_time',\n 'model_predict_time']</pre> In\u00a0[6]: Copied! <pre>from fowt_ml import LinearModels\nLinearModels.ESTIMATOR_NAMES\n</pre> from fowt_ml import LinearModels LinearModels.ESTIMATOR_NAMES Out[6]: <pre>{'LinearRegression': sklearn.linear_model._base.LinearRegression,\n 'RidgeRegression': sklearn.linear_model._ridge.Ridge,\n 'LassoRegression': sklearn.linear_model._coordinate_descent.Lasso,\n 'ElasticNetRegression': sklearn.linear_model._coordinate_descent.ElasticNet,\n 'LeastAngleRegression': sklearn.linear_model._least_angle.Lars}</pre> In\u00a0[7]: Copied! <pre># calculate metrics\nmodel_name = \"LeastAngleRegression\"\nmodel = LinearModels(model_name)\n\nmodel.use_scaled_data()\nscores = model.calculate_score(X_train, X_test, y_train, y_test, metrics) \nscores\n</pre> # calculate metrics model_name = \"LeastAngleRegression\" model = LinearModels(model_name)  model.use_scaled_data() scores = model.calculate_score(X_train, X_test, y_train, y_test, metrics)  scores Out[7]: <pre>{'neg_mean_absolute_error': -1.603052611245188,\n 'neg_root_mean_squared_error': -2.1309153094889486,\n 'r2': 0.10732388900915783,\n 'model_fit_time': np.float64(0.096),\n 'model_predict_time': np.float64(0.001)}</pre> In\u00a0[8]: Copied! <pre>scores = model.cross_validate(X_train, y_train, metrics) \nscores\n</pre> scores = model.cross_validate(X_train, y_train, metrics)  scores Out[8]: <pre>{'model_fit_time': array([0.109, 0.085, 0.081, 0.08 , 0.082]),\n 'neg_mean_absolute_error': array([-1.60136019, -1.60960767, -1.6112611 , -1.61425787, -1.59912871]),\n 'neg_root_mean_squared_error': array([-2.13342478, -2.13724123, -2.14536043, -2.15210295, -2.1258841 ]),\n 'r2': array([0.10649117, 0.10830421, 0.10762699, 0.10839059, 0.10954135]),\n 'model_predict_time': array([0.001, 0.001, 0.001, 0.001, 0.001])}</pre> In\u00a0[9]: Copied! <pre>from fowt_ml import EnsembleModel\nEnsembleModel.ESTIMATOR_NAMES\n</pre> from fowt_ml import EnsembleModel EnsembleModel.ESTIMATOR_NAMES Out[9]: <pre>{'ExtraTrees': sklearn.ensemble._forest.ExtraTreesRegressor,\n 'RandomForest': sklearn.ensemble._forest.RandomForestRegressor}</pre> In\u00a0[10]: Copied! <pre>model_name = \"RandomForest\"\nmodel = EnsembleModel(estimator=model_name, max_depth=9, bootstrap=True, max_samples=10_000, n_estimators=50)\n\nmodel.use_scaled_data()\nscores = model.calculate_score(X_train, X_test, y_train, y_test, metrics) \nscores\n</pre> model_name = \"RandomForest\" model = EnsembleModel(estimator=model_name, max_depth=9, bootstrap=True, max_samples=10_000, n_estimators=50)  model.use_scaled_data() scores = model.calculate_score(X_train, X_test, y_train, y_test, metrics)  scores Out[10]: <pre>{'neg_mean_absolute_error': -1.5959389437050893,\n 'neg_root_mean_squared_error': -2.1135488771633013,\n 'r2': 0.12112219032333066,\n 'model_fit_time': np.float64(5.78),\n 'model_predict_time': np.float64(0.003)}</pre> In\u00a0[12]: Copied! <pre>from fowt_ml import SparseGaussianModel\nSparseGaussianModel.ESTIMATOR_NAMES\n</pre> from fowt_ml import SparseGaussianModel SparseGaussianModel.ESTIMATOR_NAMES Out[12]: <pre>{'SklearnGPRegressor': fowt_ml.gaussian_process.SklearnGPRegressor}</pre> In\u00a0[13]: Copied! <pre>model_name = \"SklearnGPRegressor\"\nparams = config[\"ml_setup\"][\"model_names\"][model_name]\n\nmodel = SparseGaussianModel(\"SklearnGPRegressor\", **params)\nmodel.use_scaled_data()\nscores = model.calculate_score(X_train, X_test, y_train, y_test, metrics) \nscores\n</pre> model_name = \"SklearnGPRegressor\" params = config[\"ml_setup\"][\"model_names\"][model_name]  model = SparseGaussianModel(\"SklearnGPRegressor\", **params) model.use_scaled_data() scores = model.calculate_score(X_train, X_test, y_train, y_test, metrics)  scores Out[13]: <pre>{'neg_mean_absolute_error': -1.5981088588623225,\n 'neg_root_mean_squared_error': -2.1218047367573445,\n 'r2': 0.12320574147031532,\n 'model_fit_time': np.float64(88.066),\n 'model_predict_time': np.float64(0.021)}</pre> In\u00a0[10]: Copied! <pre>from fowt_ml import NeuralNetwork\nNeuralNetwork.ESTIMATOR_NAMES\n</pre> from fowt_ml import NeuralNetwork NeuralNetwork.ESTIMATOR_NAMES Out[10]: <pre>{'MultilayerPerceptron': sklearn.neural_network._multilayer_perceptron.MLPRegressor,\n 'RNNRegressor': fowt_ml.neural_network.SklearnRNNRegressor,\n 'LSTMRegressor': fowt_ml.neural_network.SklearnLSTMRegressor,\n 'GRURegressor': fowt_ml.neural_network.SklearnGRURegressor}</pre> In\u00a0[11]: Copied! <pre>model_name = \"MultilayerPerceptron\"\nparams = config[\"ml_setup\"][\"model_names\"][model_name]\n\nmodel = NeuralNetwork(model_name, **params)\n</pre> model_name = \"MultilayerPerceptron\" params = config[\"ml_setup\"][\"model_names\"][model_name]  model = NeuralNetwork(model_name, **params) In\u00a0[12]: Copied! <pre>model.estimator\n</pre> model.estimator Out[12]: <pre>MLPRegressor(hidden_layer_sizes=10, max_iter=50)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.MLPRegressor?Documentation for MLPRegressoriNot fitted<pre>MLPRegressor(hidden_layer_sizes=10, max_iter=50)</pre> In\u00a0[\u00a0]: Copied! <pre>model.use_scaled_data()\nscores = model.calculate_score(X_train, X_test, y_train, y_test, metrics) \nscores\n</pre> model.use_scaled_data() scores = model.calculate_score(X_train, X_test, y_train, y_test, metrics)  scores Out[\u00a0]: <pre>{'neg_mean_absolute_error': -1.5981261086400942,\n 'neg_root_mean_squared_error': -2.1186915415839853,\n 'r2': 0.12120288013830806,\n 'model_fit_time': np.float64(11.438),\n 'model_predict_time': np.float64(0.001)}</pre> In\u00a0[16]: Copied! <pre>from fowt_ml import XGBoost\nXGBoost.ESTIMATOR_NAMES\n</pre> from fowt_ml import XGBoost XGBoost.ESTIMATOR_NAMES Out[16]: <pre>{'XGBoostRegression': xgboost.sklearn.XGBRegressor}</pre> In\u00a0[17]: Copied! <pre>model_name = \"XGBoostRegression\"\nparams = config[\"ml_setup\"][\"model_names\"][model_name]\nmodel = XGBoost(model_name, **params)\n\nmodel.use_scaled_data()\nscores = model.calculate_score(X_train, X_test, y_train, y_test, metrics) \nscores\n</pre> model_name = \"XGBoostRegression\" params = config[\"ml_setup\"][\"model_names\"][model_name] model = XGBoost(model_name, **params)  model.use_scaled_data() scores = model.calculate_score(X_train, X_test, y_train, y_test, metrics)  scores Out[17]: <pre>{'neg_mean_absolute_error': -1.581722378730774,\n 'neg_root_mean_squared_error': -2.0865964889526367,\n 'r2': 0.15320807695388794,\n 'model_fit_time': np.float64(2.811),\n 'model_predict_time': np.float64(0.001)}</pre> In\u00a0[8]: Copied! <pre>from fowt_ml import NeuralNetwork\nimport numpy as np\nNeuralNetwork.ESTIMATOR_NAMES\n</pre> from fowt_ml import NeuralNetwork import numpy as np NeuralNetwork.ESTIMATOR_NAMES Out[8]: <pre>{'MultilayerPerceptron': sklearn.neural_network._multilayer_perceptron.MLPRegressor,\n 'RNNRegressor': &lt;function fowt_ml.neural_network.RNNRegressor(**args)&gt;,\n 'LSTMRegressor': &lt;function fowt_ml.neural_network.LSTMRegressor(**args)&gt;,\n 'GRURegressor': &lt;function fowt_ml.neural_network.GRURegressor(**args)&gt;}</pre> In\u00a0[9]: Copied! <pre># for torch based models, this is needed\nX_train = np.asarray(X_train, dtype=np.float32)\nX_test = np.asarray(X_test, dtype=np.float32)\ny_train = np.asarray(y_train, dtype=np.float32)\ny_test = np.asarray(y_test, dtype=np.float32)\n</pre> # for torch based models, this is needed X_train = np.asarray(X_train, dtype=np.float32) X_test = np.asarray(X_test, dtype=np.float32) y_train = np.asarray(y_train, dtype=np.float32) y_test = np.asarray(y_test, dtype=np.float32) In\u00a0[11]: Copied! <pre>model_name = \"RNNRegressor\"\nparams = {\n    \"input_size\": len(predictors_labels), \n    \"hidden_size\": 64, \n    \"output_size\": len(target_labels), \n    \"num_layers\":2,\n}\n</pre> model_name = \"RNNRegressor\" params = {     \"input_size\": len(predictors_labels),      \"hidden_size\": 64,      \"output_size\": len(target_labels),      \"num_layers\":2, } In\u00a0[12]: Copied! <pre>model = NeuralNetwork(model_name, **params)\nmodel.use_scaled_data()\n</pre> model = NeuralNetwork(model_name, **params) model.use_scaled_data() Out[12]: <pre>&lt;fowt_ml.neural_network.NeuralNetwork at 0x772ec59b3a10&gt;</pre> In\u00a0[13]: Copied! <pre>model.estimator.fit(X_train, y_train)\n</pre> model.estimator.fit(X_train, y_train) Out[13]: <pre>TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n                                                      StandardScaler()),\n                                                     ('model',\n                                                      NeuralNetRegressor(_params_to_validate={'module__num_layers', 'module__output_size', 'module__input_size', 'module__hidden_size', 'module__rnn_model'}, batch_size=128, callbacks=None, compile=False, dataset=&lt;class 'skorch.dataset.Dataset'&gt;, device='cpu', iterator_train=&lt;...4, module__input_size=14, module__num_layers=2, module__output_size=6, module__rnn_model=&lt;class 'torch.nn.modules.rnn.RNN'&gt;, optimizer=&lt;class 'torch.optim.sgd.SGD'&gt;, predict_nonlinearity='auto', torch_load_kwargs=None, train_split=&lt;skorch.dataset.ValidSplit object at 0x772ed0f63c10&gt;, use_caching='auto', verbose=0, warm_start=False))]),\n                           transformer=StandardScaler())</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.TransformedTargetRegressor?Documentation for TransformedTargetRegressoriFitted<pre>TransformedTargetRegressor(regressor=Pipeline(steps=[('scaler',\n                                                      StandardScaler()),\n                                                     ('model',\n                                                      NeuralNetRegressor(_params_to_validate={'module__num_layers', 'module__output_size', 'module__input_size', 'module__hidden_size', 'module__rnn_model'}, batch_size=128, callbacks=None, compile=False, dataset=&lt;class 'skorch.dataset.Dataset'&gt;, device='cpu', iterator_train=&lt;...4, module__input_size=14, module__num_layers=2, module__output_size=6, module__rnn_model=&lt;class 'torch.nn.modules.rnn.RNN'&gt;, optimizer=&lt;class 'torch.optim.sgd.SGD'&gt;, predict_nonlinearity='auto', torch_load_kwargs=None, train_split=&lt;skorch.dataset.ValidSplit object at 0x772ed0f63c10&gt;, use_caching='auto', verbose=0, warm_start=False))]),\n                           transformer=StandardScaler())</pre> regressor: Pipeline<pre>Pipeline(steps=[('scaler', StandardScaler()),\n                ('model',\n                 NeuralNetRegressor(_params_to_validate={'module__num_layers', 'module__output_size', 'module__input_size', 'module__hidden_size', 'module__rnn_model'}, batch_size=128, callbacks=None, compile=False, dataset=&lt;class 'skorch.dataset.Dataset'&gt;, device='cpu', iterator_train=&lt;class 'torch.utils.data.dataloader.Dat...Module'&gt;, module__hidden_size=64, module__input_size=14, module__num_layers=2, module__output_size=6, module__rnn_model=&lt;class 'torch.nn.modules.rnn.RNN'&gt;, optimizer=&lt;class 'torch.optim.sgd.SGD'&gt;, predict_nonlinearity='auto', torch_load_kwargs=None, train_split=&lt;skorch.dataset.ValidSplit object at 0x772ed0f63c10&gt;, use_caching='auto', verbose=0, warm_start=False))])</pre> StandardScaler?Documentation for StandardScaler<pre>StandardScaler()</pre> NeuralNetRegressor<pre>&lt;class 'skorch.regressor.NeuralNetRegressor'&gt;[uninitialized](\n  module=&lt;class 'fowt_ml.neural_network.GenericRNNModule'&gt;,\n  module__hidden_size=64,\n  module__input_size=14,\n  module__num_layers=2,\n  module__output_size=6,\n  module__rnn_model=&lt;class 'torch.nn.modules.rnn.RNN'&gt;,\n)</pre> transformer: StandardScaler<pre>StandardScaler()</pre> StandardScaler?Documentation for StandardScaler<pre>StandardScaler()</pre> In\u00a0[\u00a0]: Copied! <pre>scores = model.calculate_score(X_train, X_test, y_train, y_test, metrics) \nscores\n</pre> scores = model.calculate_score(X_train, X_test, y_train, y_test, metrics)  scores <pre>  epoch    train_loss    valid_loss      dur\n-------  ------------  ------------  -------\n      1        0.9143        0.8909  24.6896\n      2        0.8943        0.8866  26.2002\n      3        0.8915        0.8850  33.9241\n      4        0.8902        0.8840  36.8488\n      5        0.8893        0.8833  38.8893\n      6        0.8886        0.8827  37.3335\n      7        0.8881        0.8823  40.8689\n      8        0.8876        0.8819  36.2290\n      9        0.8872        0.8815  36.1466\n     10        0.8869        0.8812  40.8171\n</pre> Out[\u00a0]: <pre>{'neg_mean_absolute_error': -1.6005001068115234,\n 'neg_root_mean_squared_error': -2.124774694442749,\n 'r2': 0.11344795674085617,\n 'model_fit_time': np.float64(352.913),\n 'model_predict_time': np.float64(0.001)}</pre> In\u00a0[10]: Copied! <pre>model_name = \"LSTMRegressor\"\nparams = {\n    \"input_size\": len(predictors_labels), \n    \"hidden_size\": 64, \n    \"output_size\": len(target_labels), \n    \"num_layers\":2,\n    \"max_epochs\":5,\n}\n</pre> model_name = \"LSTMRegressor\" params = {     \"input_size\": len(predictors_labels),      \"hidden_size\": 64,      \"output_size\": len(target_labels),      \"num_layers\":2,     \"max_epochs\":5, } In\u00a0[11]: Copied! <pre>model = NeuralNetwork(model_name, **params)\nmodel.use_scaled_data()\nscores = model.calculate_score(X_train, X_test, y_train, y_test, metrics) \nscores\n</pre> model = NeuralNetwork(model_name, **params) model.use_scaled_data() scores = model.calculate_score(X_train, X_test, y_train, y_test, metrics)  scores <pre>  epoch    train_loss    valid_loss     dur\n-------  ------------  ------------  ------\n      1        1.0000        0.9892  6.0975\n      2        0.9963        0.9848  6.2456\n      3        0.9905        0.9776  6.3866\n      4        0.9819        0.9680  6.6953\n      5        0.9717        0.9581  7.1940\n</pre> Out[11]: <pre>{'neg_mean_absolute_error': -1.6250715255737305,\n 'neg_root_mean_squared_error': -2.1550610065460205,\n 'r2': 0.03436806797981262,\n 'model_fit_time': np.float64(33.554),\n 'model_predict_time': np.float64(0.001)}</pre> In\u00a0[19]: Copied! <pre>model_name = \"GRURegressor\"\nparams = {\n    \"input_size\": len(predictors_labels), \n    \"hidden_size\": 64, \n    \"output_size\": len(target_labels), \n    \"num_layers\":2,\n    \"max_epochs\": 5,\n}\n</pre> model_name = \"GRURegressor\" params = {     \"input_size\": len(predictors_labels),      \"hidden_size\": 64,      \"output_size\": len(target_labels),      \"num_layers\":2,     \"max_epochs\": 5, } In\u00a0[20]: Copied! <pre>model = NeuralNetwork(model_name, **params)\nmodel.use_scaled_data()\nscores = model.calculate_score(X_train, X_test, y_train, y_test, metrics) \nscores\n</pre> model = NeuralNetwork(model_name, **params) model.use_scaled_data() scores = model.calculate_score(X_train, X_test, y_train, y_test, metrics)  scores <pre>  epoch    train_loss    valid_loss      dur\n-------  ------------  ------------  -------\n      1        0.9858        0.9623  81.2824\n      2        0.9600        0.9410  107.4753\n      3        0.9390        0.9225  109.0925\n      4        0.9237        0.9114  115.3645\n      5        0.9143        0.9039  117.8505\n</pre> Out[20]: <pre>{'neg_mean_absolute_error': -1.6065346002578735,\n 'neg_root_mean_squared_error': -2.1341967582702637,\n 'r2': 0.08973821997642517,\n 'model_fit_time': np.float64(531.139),\n 'model_predict_time': np.float64(0.001)}</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"notebooks/examples_models/#how-to-run-a-model-using-fowt_ml-and-calculate-scores","title":"How to run a model using <code>fowt_ml</code> and calculate scores\u00b6","text":""},{"location":"notebooks/examples_models/#data-preparation-common-for-any-type-of-model","title":"Data preparation (common for any type of model)\u00b6","text":""},{"location":"notebooks/examples_models/#linear-models","title":"Linear models\u00b6","text":""},{"location":"notebooks/examples_models/#random-forest","title":"Random Forest\u00b6","text":""},{"location":"notebooks/examples_models/#gaussian-process","title":"Gaussian Process\u00b6","text":""},{"location":"notebooks/examples_models/#mlp","title":"MLP\u00b6","text":""},{"location":"notebooks/examples_models/#xgboots","title":"XGBoots\u00b6","text":""},{"location":"notebooks/examples_models/#rnn-models","title":"RNN models\u00b6","text":""}]}